{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4184713b-3d12-4e5b-abc8-d9b85922f01e",
   "metadata": {},
   "source": [
    "# Intro to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca111e-9330-43db-8b1a-d13712013a2f",
   "metadata": {},
   "source": [
    "**Disclaimer:** TP plus compliqué !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806be423-194a-4a2e-8403-2435353e5c3e",
   "metadata": {},
   "source": [
    "**Spark** est un système de calcul hautement parallélisé :\n",
    "- au niveau du stockage : la data est fragmentée, dupliquée, répartie sur un nombre quelconque de disques/servers\n",
    "- au niveau du calcul : les calculs s'exécutent en parallèle sur plusieurs machines, chacune sur sa portion de data\n",
    "\n",
    "Spark en tant que tel est un framework qui a des implémentation dans plusieurs langages :\n",
    "- Python via PySpark (ce qe nous utiliserons)\n",
    "- Scala\n",
    "- R\n",
    "- Java\n",
    "\n",
    "## Quelques informations en vrac sur Spark\n",
    "### Hardware\n",
    "- Spark peut s'installer sur un grand nombre de machines situées dans un même réseau. Elles pourront ensuite se reconnaître et collaborer. 1 unité de calcul = 1 noeud.\n",
    "- Spark fonctionne sur le mode master/worker : un noeud est désigné `master` et jouera le rôle de chef d'orchestre pour que les autres noeuds `workers` exécutent les tâches dans le bon ordre\n",
    "- Les noeuds Spark communiquent énormément entre eux pour s'échanger des informations et surtout des données\n",
    "\n",
    "### Software\n",
    "- Un code Spark / PySpark doit utiliser les primitives Spark pour que tout s'exécute selon la logique Spark\n",
    "- Le code pyspark est transmis au noeud `master` qui le lit et prépare l'orchestration des calculs selon les `workers` qu'il a à disposition. Seuls les `workers` manipuleront la donnée (sauf exception)\n",
    "- Spark est *lazy* : `master` ne lance réellement aucun calcul tant qu'il n'a pas lu d'opération impliquant l'affichage ou l'écriture des résultats\n",
    "- Corrolaire du *lazy* : sans précaution, Spark peut répéter plusieurs fois les mêmes calculs ... Exemple avec 2 chaînes de transformation data `A -> B -> C -> D` suivi de `A -> B -> E`. Les étapes intermédiaires `A -> B` sont identiques mais pour calculer `D` et `E`, Spark risque de les exécuter 2 fois. Apprendre à manipuler les méthodes [cache](https://spark.apache.org/docs/3.5.3/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.cache.html?highlight=cache) !\n",
    "\n",
    "### Framework\n",
    "- Spark se base sur des `DataFrame` très proches en terme d'utilisation des `pandas.DataFrame` donc pas de panique :)\n",
    "- Spark gère tout ce que sait gérer SQL mais part de fichiers plats (ici CSV) : join, select, etc ...\n",
    "- PySpark permet de gérer tout ces concepts avec la facilité d'accès du Python\n",
    "- PySpark est TRÈS typé et a besoin de connaître les types de chaque colonne manipulées\n",
    "- Spark est TRÈS flexible en terme de configuration et d'exécution et cela peut sembler déroutant pour des exemples simples\n",
    "\n",
    "### Organisation du calcul\n",
    "En informatique, à chaque architecture ses optimisations :\n",
    "- en local sur 1 CPU, un calcul possède peu d'optimisation : simple et linéaire, possiblement async\n",
    "- en local sur multi CPU, un calcul doit être prévu pour paralléliser les exécutions sur plusieurs coeurs physiques\n",
    "- en local sur mono/multi GPU, un calcul doit être hautement parallélisable, découpable en tranche de data qui tiennent en GPU-RAM\n",
    "- en multi machine multi GPU, idem que plus haut avec bande passante importante entre machine pour échange d'information\n",
    "\n",
    "... Spark gère le multi machine, multi CPU, multi RAM, multi disque : calculs hautement parallélisés grâce à la magie de Spark, data échangées au mieux entre machine (possiblement avec l'aide humaine).\n",
    "__Spark est toujours prêt à gérer un contexte d'exécution très complexe__ => il faut s'attendre à beaucoup d'overhead sur des cas simples\n",
    "\n",
    "**Les opérations s'exécutent sur des workers séparés, en parallèle**, il faut donc parfois faire \"un peu attention\" à la façon dont on demande à Spark de *partitionner* sa data.\n",
    "\n",
    "## À retenir\n",
    "\n",
    "1. Spark et son implémentation PySpark sont très puissant car gèrent un parallélisme quasi infini et réglable à 100%\n",
    "2. PySpark a un coût d'entrée pour se couler dans le moule Spark mais permet de réaliser des opérations très complexes avec la simplicité du Python\n",
    "3. Votre notebook n'exécutera n'a pas accès à la data manipulée et n'effectuera aucun calculs ; il les transmettra au Spark Master qui les répartira entre ses workers qui ont accès à la data\n",
    "\n",
    "## Exemple de code Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f318386f-71ef-44ec-aaf9-160ecd6ff0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ea6256-c381-4264-99b8-8987b21e8062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/14 07:49:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize PySpark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JupyterHub PySpark Example\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate() \n",
    "\n",
    "# /!\\ Tout se fera à partir de cet object magique `spark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4fccced-12e3-4a36-a9d4-7884eab014ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     Name|Value|\n",
      "+---------+-----+\n",
      "|    Alice|    1|\n",
      "|      Bob|    2|\n",
      "|Catherine|    3|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Catherine\", 3)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Value\"])\n",
    "\n",
    "# Show the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0328d481-2517-43d6-af84-935714cc9197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, FloatType, ArrayType, StructField, StructType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "365084ed-4817-426d-919d-13f827892be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(id='1', brewery_id='812', name='Hocus Pocus', cat_id='11', style_id='116', abv='4.5', ibu='0', srm='0', upc='0', filepath=None, descript='Our take on a classic summer ale.  A toast to weeds, rays, and summer haze.  A light, crisp ale for mowing lawns, hitting lazy fly balls, and communing with nature, Hocus Pocus is offered up as a summer sacrifice to clodless days.', add_user=None, last_mod=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_beers = spark.read.csv(\"/datasets/csv/beers.csv\", header=True)\n",
    "df_beers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5194190e-50b3-4e2d-a44c-24ded27a9e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                name| reverse_capitalized|\n",
      "+--------------------+--------------------+\n",
      "|         Hocus Pocus|         SUCOP SUCOH|\n",
      "| 2010-07-22 20:00:20| 02:00:02 22-70-0102|\n",
      "|   Grimbergen Blonde|   EDNOLB NEGREBMIRG|\n",
      "|Widdershins Barle...|ENIWYELRAB SNIHSR...|\n",
      "|             Lucifer|             REFICUL|\n",
      "|              Bitter|              RETTIB|\n",
      "|       Winter Warmer|       REMRAW RETNIW|\n",
      "|Winter Welcome 20...|8002-7002 EMOCLEW...|\n",
      "|       Oatmeal Stout|       TUOTS LAEMTAO|\n",
      "|     Espresso Porter|     RETROP OSSERPSE|\n",
      "|     Chocolate Stout|     TUOTS ETALOCOHC|\n",
      "|Hitachino Nest Re...|WERB REGNIG LAER ...|\n",
      "|         JuJu Ginger|         REGNIG UJUJ|\n",
      "|      The Kidd Lager|      REGAL DDIK EHT|\n",
      "|      Imperial Stout|      TUOTS LAIREPMI|\n",
      "|Oak-Aged Belgian ...|LEPIRT NAIGLEB DE...|\n",
      "|         Ultrablonde|         EDNOLBARTLU|\n",
      "|  Wiesen Edel Weisse|  ESSIEW LEDE NESEIW|\n",
      "|    Old Foghorn 2001|    1002 NROHGOF DLO|\n",
      "|           Framboise|           ESIOBMARF|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define a specific funtion to map beer names\n",
    "@F.udf(returnType=StringType())\n",
    "def revert_cap_name(name: str):\n",
    "    return name[::-1].upper()\n",
    "\n",
    "\n",
    "df_beers.withColumn(\"reverse_capitalized\", revert_cap_name(F.col(\"name\"))).select(\"name\", \"reverse_capitalized\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d25580a-421d-41c6-bdb8-6ebe81a4b39b",
   "metadata": {},
   "source": [
    "## Observations\n",
    "Que remarque-t-on tout de suite ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9672354a-2827-42fc-a265-fcedd0e8affc",
   "metadata": {},
   "source": [
    "# Uses cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19828ca-a8ba-4727-8258-b748ce8afb83",
   "metadata": {},
   "source": [
    "# UC-1 : description data\n",
    "\n",
    "- Q1: Combien y a-t-il de bières dans la DB ?\n",
    "- Q2: Top10 brasseries les plus représentées avec le nombre de bière par brasserie ?\n",
    "- Q3: Top10 des bières les plus fortes (ABV) en France ?\n",
    "- Q4: Par pays, nombre de brasseries qui proposent des bières de type `Porter` et ABV moyen de celles-ci ?\n",
    "- Q5: Mediane du nombre de bière par pays ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d79a3df-f2c8-42aa-b175-edb4269b945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beers = spark.read.csv(\"/datasets/csv/beers.csv\", header=True)\n",
    "df_breweries = spark.read.csv(\"/datasets/csv/breweries.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14582ac5-f3a8-4ba6-b4e5-16c354d33db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: 7060 dans la DB\n",
      "CPU times: user 2.67 ms, sys: 323 μs, total: 2.99 ms\n",
      "Wall time: 663 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "n_beers = df_beers.count()\n",
    "print(f\"Q1: {n_beers} dans la DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7e49de9-688e-42ce-8599-a50fa82a89bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|       country|count|\n",
      "+--------------+-----+\n",
      "| United States| 4552|\n",
      "|       Belgium|  331|\n",
      "|       Germany|  302|\n",
      "|United Kingdom|  210|\n",
      "|        Canada|  156|\n",
      "|   Netherlands|   29|\n",
      "|   Switzerland|   25|\n",
      "|       Austria|   25|\n",
      "|     Australia|   24|\n",
      "|        Norway|   22|\n",
      "+--------------+-----+\n",
      "\n",
      "CPU times: user 7.49 ms, sys: 478 μs, total: 7.97 ms\n",
      "Wall time: 1.78 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Q2\")\n",
    "dd = (df_beers\n",
    "      .join(df_breweries, on=df_beers.brewery_id == df_breweries.id)\n",
    "      .groupby(\"country\")\n",
    "      .count()\n",
    "      .sort(F.col(\"count\").desc())\n",
    "      .limit(10)\n",
    ")\n",
    "dd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34d869dd-b1a4-4dda-b5b1-e0d8a050cc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/14 07:50:01 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|                name|abv_float|country|\n",
      "+--------------------+---------+-------+\n",
      "|           Belzebuth|     13.0| France|\n",
      "|Gavroche French R...|      8.5| France|\n",
      "|             3 Monts|      8.5| France|\n",
      "|                Yeti|      8.0| France|\n",
      "|      Jenlain Blonde|      7.5| France|\n",
      "|              Blonde|      7.5| France|\n",
      "|   Les Sans Culottes|      7.0| France|\n",
      "|           Framboise|      6.0| France|\n",
      "|Jenlain St Druon ...|      6.0| France|\n",
      "|Castelain St.Aman...|      5.9| France|\n",
      "+--------------------+---------+-------+\n",
      "\n",
      "CPU times: user 10.7 ms, sys: 4.27 ms, total: 14.9 ms\n",
      "Wall time: 2.35 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Q3\n",
    "\n",
    "@F.udf(returnType=FloatType())\n",
    "def safe_cast_to_float(str_float: str):\n",
    "    return float(str_float)\n",
    "\n",
    "df_beers_brewers = (\n",
    "    df_beers\n",
    "    .join(df_breweries.withColumnRenamed(\"name\", \"brewer_name\"), on=df_beers.brewery_id == df_breweries.id)\n",
    ").cache()\n",
    "\n",
    "print(\"Q3\")\n",
    "dd = (df_beers_brewers\n",
    "      .filter(F.col(\"country\") == F.lit(\"France\"))\n",
    "      .withColumn(\"abv_float\", safe_cast_to_float(F.col(\"abv\")))\n",
    "      .sort(F.col(\"abv_float\").desc())\n",
    "      .select([\"name\", \"abv_float\", \"country\"])\n",
    "      .limit(10)\n",
    ")\n",
    "dd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2592486f-5350-4da7-bca6-3188c087d868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4\n",
      "+--------------+------------------+----------------------+\n",
      "|       country|           avg_abv|n_brewer_having_porter|\n",
      "+--------------+------------------+----------------------+\n",
      "|        Russia|               7.0|                     1|\n",
      "|        Sweden|               5.5|                     1|\n",
      "|       Germany| 7.099999904632568|                     1|\n",
      "| United States|2.3305555541291194|                   216|\n",
      "|     Lithuania| 6.800000190734863|                     1|\n",
      "|        Norway|               7.0|                     1|\n",
      "|       Denmark|               8.0|                     1|\n",
      "|   Switzerland|               4.5|                     1|\n",
      "|        Canada|               0.0|                     5|\n",
      "|Czech Republic|               8.0|                     1|\n",
      "|         Japan|               0.0|                     1|\n",
      "|        Poland| 8.300000190734863|                     1|\n",
      "|     Australia|               0.0|                     1|\n",
      "|United Kingdom|3.9500000136239186|                    13|\n",
      "+--------------+------------------+----------------------+\n",
      "\n",
      "CPU times: user 5.89 ms, sys: 8.08 ms, total: 14 ms\n",
      "Wall time: 1.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Q4\")\n",
    "df_style = spark.read.csv(\"/datasets/csv/styles.csv\", header=True)\n",
    "target_style_id = df_style.filter(F.lower(F.col(\"style_name\")) == \"porter\").select(F.col(\"id\").alias(\"style_id\"))\n",
    "dd = (\n",
    "    df_beers_brewers\n",
    "    .join(target_style_id, how=\"inner\", on=\"style_id\")\n",
    "    .withColumn(\"abv_float\", safe_cast_to_float(F.col(\"abv\")))\n",
    "    .select([\"name\", \"brewer_name\", \"abv_float\", \"country\"])\n",
    "    .groupby(\"country\")\n",
    "    .agg(F.avg(\"abv_float\").alias(\"avg_abv\"), F.countDistinct(\"brewer_name\").alias(\"n_brewer_having_porter\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0d25f07-0333-4845-a781-b8e023ec2481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[country: string, count: bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = (\n",
    "    df_beers_brewers\n",
    "    .groupby(\"country\")\n",
    "    .count()\n",
    ")\n",
    "dd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3a391c9-33e1-48f4-9afe-4d3c727261cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:====================================================> (195 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5: 3.0\n",
      "CPU times: user 15.2 ms, sys: 5.4 ms, total: 20.6 ms\n",
      "Wall time: 5.87 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Q5:\", dd.agg(F.median(\"count\")).first()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfed943-eba2-4563-9de9-adcc889c53a7",
   "metadata": {},
   "source": [
    "Voyons la différence de vitesse avec une version Numpy local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39ba7215-1d6a-4b18-b73d-ebd13eb00278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "obs = dd.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1c52c1b-2e34-4cf7-b8cd-a20aca15be68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5 version local via Numpy: 3.0\n",
      "CPU times: user 1.64 ms, sys: 209 μs, total: 1.85 ms\n",
      "Wall time: 1.71 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Q5 version local via Numpy:\", np.median(obs[\"count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82d2e7-274a-49e8-bb1f-5d491bec2609",
   "metadata": {},
   "source": [
    "# UC-2 : préparer un dataset de ranking \n",
    "Tout moteur de recherche/search-engine - **SE** - nécessite de la configuration ... beaucoup de configuration. Une des configuration très orientée \"data\" est le calcul que l'index doit opérer pour scorer chaque réponse possible face à une requête. L'apprentissage statistique de ce score s'appelle *Learning to Rank*  - **LTR** - et nécessite des connaissances poussées en machine learning. \n",
    "\n",
    "Cette tâche LTR se base sur les *feedbacks implicites* des utilisateurs face au moteur de recherche. Commençons par un exemple. Quand vous cherchez un objet sur LeBonCoin, vous laissez plusieurs informations *implicites* sur votre perception des résultats proposés : les item sur lesquels vous avez cliqués bien sûr mais également ceux que vous avez probablement *vu* sans cliquer dessus ... Ces \"vues sans clics\" sont une précieuse information implicite sur les jugement que vous avez porté aux résultats proposés. Pour ce TP nous nous limiterons à ce concept de \"vu x click\" mais il est possible d'aller plus loin (dwell-time, hierarchisation des interactions explicites, ...). \n",
    "\n",
    "On appelle *Search Engine Results Page* - **SERP** - la liste des résultats classés par un SE. Un document qui figure dans les résulats d'une recherche a donc une position (son rang) au sein de la **SERP**.\n",
    "\n",
    "Exemple, où :\n",
    "- `query` est la recherche réalisée par un user et qui a débouché sur une SERP\n",
    "- `clicked_id` : l'id de la bière cliquée par le user\n",
    "- `user_id` l'id de l'utilisateur (simplifions en disant que c'est même l'id d'une recherche) : permet de retrouver tous les résultats proposés dans **une** recherche\n",
    "- `id_in_serp` : l'id d'une bière figurant dans la SERP\n",
    "- `pos_in_serp` : la position/le rang de la bière `id_in_serp` dans la SERP issue de la recherche "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2153f482-d334-4f46-8134-d25dfe0643a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+----------+-----------+\n",
      "|      query|clicked_id|             user_id|id_in_serp|pos_in_serp|\n",
      "+-----------+----------+--------------------+----------+-----------+\n",
      "|fruity sour|      4442|ecfce536-7fc5-11e...|      4442|          1|\n",
      "|fruity sour|      4442|ecfce536-7fc5-11e...|       475|          2|\n",
      "|fruity sour|      4442|ecfce536-7fc5-11e...|       481|          3|\n",
      "+-----------+----------+--------------------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_pref = spark.read.csv(\"/datasets/beers_feedback.csv\", header=True, inferSchema=True)\n",
    "df_pref.limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28882f0-de9a-4fbc-97c5-a37864103477",
   "metadata": {},
   "source": [
    "Un travail préliminaire au LTR est la constitution d'un dataset qui permet d'aggréger ces feedbacks laissés par tous les utilisateurs ayant réalisé la même query. Chacun a vu et cliqué selon ses propres impressions de pertinence et il convient de \"moyenner\" tout cela pour obtenir des appréciations globales. L'objectif d'un tel dataset est de pouvoir lister des exemples de triplets `(query, document, note)` qui permet de savoir que face à une *query* `milky stout low bitterness`, un *document* `Super bitter beer brewed with organic roasted barley and chocolate` aura une pertinence de *1/4* (arbitraire). \n",
    "\n",
    "Implémenter le modèle d'agrégation de feedback \"cascade model\" [1] (pour la culture, **inutile d'avoir lu l'article** pour le TD) qui propose une approche pragmatique pour obtenir ces données. La méthode est la suivante :\n",
    "- pour chaque recherche utilisateur:\n",
    "    - étudier la position de l'id cliqué dans la SERP - soit `clicked_pos_in_serp` cette information\n",
    "    - Considérer que tout doc situés \"au-dessus dans la SERP\" (càd quand `pos_in_serp <= clicked_pos_in_serp`) avait été vu par l'utilisateur\n",
    "    - Récapituler tous ces documents \"vus et cliqués\" et \"vus mais pas cliqués\"\n",
    "- Pour chaque recherche et bière cliquée (`clicked_id`), calculer la \"probabilité de clic sachant qu'elle a été vue\", càd le nombre de fois qu'elle a été cliquée divisé par le nombre de fois où elle a été vue\n",
    "\n",
    "\n",
    "[1] https://dl.acm.org/doi/abs/10.1145/1341531.1341545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3945da0-7065-4872-a82e-54df5b84f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a1646b-d653-496a-be37-0d425c1abadd",
   "metadata": {},
   "source": [
    "# UC-3 : récupérer les docs qui parlent d'un mot\n",
    "\n",
    "Peut-on utiliser SQL pour réaliser un mini moteur de recherche ? Pour différentes requêtes (`query` en anglais) textuelles très simples à base de mot-clef, retrouver les bières qui semblent répondre à la demande. Exemples :\n",
    "- trouver les bières ou les brasseries qui parlent de bières \"fine\"\n",
    "- idem pour \"juicy\"\n",
    "- idem pour \"genuine\"\n",
    "- idem pour les bières mâturées dans des \"oak cask\" (fûts en chêne) -> combien y en a-t-il ? $N_1$\n",
    "   - idem pour les bières qui évoquent uniquement \"cask\" -> combien y en a-t-il ? $N_{1,1}$\n",
    "   - idem pour celles ne parlant que de \"oak\" -> combien y en a-t-il ? $N_{1,2}$\n",
    "- idem pour les bières qui évoquent \"oak\" et \"cask\" -> combien y en a-t-il ? $N_{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "675fd4dd-8135-4f27-a0cb-2c8baeb1df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d942b-9c0d-46b5-afa1-b46dfcb4393f",
   "metadata": {},
   "source": [
    "# UC-4 : vectorisation des description des bières\n",
    "Préparer le recours à un service de vectorisation qui permettra de convertir la connaissance sur une bière en un vecteur numérique. Ce vecteur permet de sythétiser mathématiquement l'information disponible sur une bière et sa brasserie et pourra être réutilisé plus tard dans un moteur de recherche.\n",
    "à faire :\n",
    "- Préparer une description la plus complète possible pour chaque bière\n",
    "- envoyer ces descriptions une à une via un appel HTTP sur Jina (voir instruction plus bas)\n",
    "\n",
    "**Découpez le travail** : chacun travaillera sur un sous-ensemble de bières selon l'`id` de chaque bière `beers.id`. \n",
    "Vous êtes 12, je propose donc la répartition suivante :\n",
    "- ADAM.LUCAS --> s'occuper des `beers.id` égaux à 0 modulo 12\n",
    "- ALIEINIK.OLHA --> s'occuper des `beers.id` égaux à 1 modulo 12\n",
    "- ARNOUT.FABRICE --> s'occuper des `beers.id` égaux à 2 modulo 12\n",
    "- BEDIER.DORIANE --> s'occuper des `beers.id` égaux à 3 modulo 12\n",
    "- CASTRO.MOUCHERON --> s'occuper des `beers.id` égaux à 4 modulo 12\n",
    "- COLIN.KEVIN --> s'occuper des `beers.id` égaux à 5 modulo 12\n",
    "- FRASELLE.NADEGE --> s'occuper des `beers.id` égaux à 6 modulo 12\n",
    "- KUKSA.OLEKSANDRA --> s'occuper des `beers.id` égaux à 7 modulo 12\n",
    "- LOPES.VAZ.ALEXIS --> s'occuper des `beers.id` égaux à 8 modulo 12\n",
    "- REITER.ROMAIN --> s'occuper des `beers.id` égaux à 9 modulo 12\n",
    "- RICHIER.MARCUS --> s'occuper des `beers.id` égaux à 10 modulo 12\n",
    "- VINOT.MATHIEU --> s'occuper des `beers.id` égaux à 11 modulo 12\n",
    "\n",
    "## Service de vectorisation Jina\n",
    "Nous allons faire appel à un service de vectorisation externe [https://jina.ai](https://jina.ai) qui propose gratuitement 1M token de vectorisation. \n",
    "Quand vous voudrez vectoriser un texte, suivez la doc de [https://jina.ai/embeddings/](https://jina.ai/embeddings/). \n",
    "\n",
    "Nous utiliserons **TOUS le MÊME modèle d'embedding** : `jina-embeddings-v2-base-en` ! Faites donc attention à appeler le bon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b1cfee-f1dc-4842-bd80-c30f4d1c57b3",
   "metadata": {},
   "source": [
    "Essayons de construire d'avoir tous le même schéma de texte à vectoriser :\n",
    "`the beer BEER_NAME from brewery BREWERY_NAME (BREWERY_DESCRIPTION) is defined as BEER_DESCRIPTION. Spec of the beer are: ABV=ABV_VALUE, IBU=IBU_VALUE, SRM=SRM_VALUE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ee4db-2551-41b8-aa2c-e62e00f29f6d",
   "metadata": {},
   "source": [
    "#### Instructions pour appeler le service Jina\n",
    "En plus de la doc sur leur site, voici un snippet de code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5b09324-7f15-4e5f-a855-79973a7f9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "EMBEDDING_NAME = \"jina-embeddings-v2-base-en\"\n",
    "url = 'https://api.jina.ai/v1/embeddings'\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': 'Bearer jina_85ba1ab9e5ff4017b3d216ebb8734f27xzJ9WyoYBFwqks9lOaNLHryw_Yyz'\n",
    "}\n",
    "\n",
    "sentences_to_vec = [\"Hi i'm a student at Université de Lorraine\", \"This is big data workshop\"]\n",
    "data = {\n",
    "    'model': EMBEDDING_NAME,\n",
    "    'normalized': True,\n",
    "    'embedding_type': 'float',\n",
    "    'input': sentences_to_vec\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7140ff98-3760-4d68-9d47-b2410d3107b7",
   "metadata": {},
   "source": [
    "Rappel de la classe `JinaEmbedder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83d2f2-76b7-4ba9-9523-a8a75e1d45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from rich.progress import track, Progress\n",
    "\n",
    "def batched(iterable, batch_size=16):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, batch_size):\n",
    "        yield iterable[ndx:min(ndx + batch_size, l)]\n",
    "\n",
    "class JinaEmbedder:\n",
    "    \n",
    "    URL = 'https://api.jina.ai/v1/embeddings'\n",
    "    EMBEDDING_NAME = \"jina-embeddings-v2-base-en\"\n",
    "    bearer_token = 'Bearer jina_85ba1ab9e5ff4017b3d216ebb8734f27xzJ9WyoYBFwqks9lOaNLHryw_Yyz'\n",
    "\n",
    "    @staticmethod\n",
    "    def http_json_to_vec(http_json: dict):\n",
    "        return np.array(\n",
    "            [\n",
    "                sentence[\"embedding\"]\n",
    "                for sentence in http_json[\"data\"]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _get_header(cls) -> dict:\n",
    "        return {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Authorization': cls.bearer_token\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def _embed_one_batch(cls, batch: List[str]) -> requests.Response:\n",
    "        headers = cls._get_header()\n",
    "        data = {\n",
    "            'model': cls.EMBEDDING_NAME,\n",
    "            'normalized': True,\n",
    "            'embedding_type': 'float',\n",
    "            'input': batch\n",
    "        }\n",
    "        \n",
    "        return requests.post(cls.URL, headers=headers, json=data)\n",
    "\n",
    "    @classmethod\n",
    "    def embed(cls, str_to_vectorize: List[str] | str, batch_size=256) -> np.ndarray:\n",
    "        if isinstance(str_to_vectorize, str):\n",
    "            str_to_vectorize = [str_to_vectorize]\n",
    "\n",
    "        embeddings = []\n",
    "        with Progress() as progress:\n",
    "            for i, batch in progress.track(enumerate(batched(str_to_vectorize, batch_size=batch_size))):\n",
    "                progress.print(f\"batch {i}...\")\n",
    "                response = cls._embed_one_batch(batch)\n",
    "        \n",
    "                if (sc:=response.status_code) != 200:\n",
    "                    print(\"Warning ! Batch\", i, \"has status code\", sc, \"-> skipping\")\n",
    "                    embeddings.append(np.array([None]*len(batch)))\n",
    "                else:\n",
    "                    embeddings.append(JinaEmbedder.http_json_to_vec(response.json()))\n",
    "        return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "810d106a-bcd8-4d46-9200-c7d6bd88797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee22f37-b07e-4f63-935e-f153e680f089",
   "metadata": {},
   "source": [
    "# UC-5 : answer question in corpa\n",
    "\n",
    "**Question difficile en Spark**\n",
    "\n",
    "**Grandes lignes :** trouvons les documents qui répondent à une question. Exemple : à partir de la description vectorisée à UC-4 pour chaque bière, comment trouver les bières qui répondent à une description plus complète ? Exemple:\n",
    "- \"very bitter beer with smoky taste\"\n",
    "- \"fruity sour - balanced sourness\"\n",
    "- \"weird beer\"\n",
    "\n",
    "Voir la doc [Spark ML lib - feature extraction](https://spark.apache.org/docs/latest/api/python/reference/pyspark.mllib.html#feature) pour trouver des idées (TF-IDF, Word2Vec), ou utiliser le résultats de vos vectorisation de UC-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3257601-6abd-403f-9a13-3f59e2f1973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"very bitter beer with smoky taste\", \"fruity sour - balanced sourness\", \"weird beer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf797414-dea8-42c6-bb70-686167b0957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "585b3ebb-d0f0-4949-9777-a827d53b8914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|              to_vec|\n",
      "+---+--------------------+\n",
      "|100|The rewery Nebras...|\n",
      "|529|The rewery Brasse...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_corpus = (\n",
    "    df_description\n",
    "    .withColumn(\"to_vec\", craft_to_txt_vectorize(\"beer_name\", \"brewer_name\", \"beer_text\", \"brewer_text\", \"abv\", \"ibu\", \"srm\"))\n",
    "    .repartition(2)\n",
    "    .select(\"id\", \"to_vec\")\n",
    ")\n",
    "df_corpus.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48addc14-0bca-4c7f-af3b-f0e2c532bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "def create_tf_and_idf_from_corpus(df_corpus):\n",
    "    tf = HashingTF(inputCol=\"tokenized\", outputCol=\"raw_features\")\n",
    "    df = tf.transform(df_corpus.withColumn(\"tokenized\", F.split(\"to_vec\", \" \")))\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\").fit(df)\n",
    "    return tf, idf\n",
    "\n",
    "def turn_to_tf_idf(df_docs, tf, idf):\n",
    "    docs_tf = tf.transform(df_docs.withColumn(\"tokenized\", F.split(\"to_vec\", \" \")))\n",
    "    docs_tfidf = idf.transform(docs_tf)\n",
    "    return docs_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "267cf58e-7b18-4809-9a5f-6ce58db1e7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Pre compute TF and IDF\n",
    "tf, idf = create_tf_and_idf_from_corpus(df_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c382782a-890d-4584-9cef-668525c98f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply on corpus and queries\n",
    "corpus_tfidf = turn_to_tf_idf(df_corpus, tf, idf)\n",
    "\n",
    "queries_df = spark.createDataFrame(pd.DataFrame(data={\"to_vec\": queries}))\n",
    "queries_tfidf = turn_to_tf_idf(queries_df, tf, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "baa83524-dfdb-424d-bae6-6a0f461a2cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/14 08:01:40 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# broadcast queries to every partitions\n",
    "broadcast_queries_tfidf = spark.sparkContext.broadcast(queries_tfidf.select([\"features\", \"tokenized\"]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "305dca7b-e998-4bfc-a9c2-81a5826d2a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to compute dot product\n",
    "def compute_dot_product(doc_vec, query_vec):\n",
    "    #if isinstance(doc_vec, SparseVector):\n",
    "    doc_vec = doc_vec.toArray()\n",
    "    #if isinstance(query_vec, SparseVector):\n",
    "    query_vec = query_vec.toArray()\n",
    "    return float(doc_vec.dot(query_vec))\n",
    "\n",
    "# Register UDF for dot product\n",
    "dot_product_udf = F.udf(compute_dot_product, DoubleType())\n",
    "\n",
    "# Explode the broadcasted queries_df into rows\n",
    "queries_rdd = spark.sparkContext.parallelize(broadcast_queries_tfidf.value)\n",
    "queries_df_expanded = spark.createDataFrame(queries_rdd)\n",
    "\n",
    "# Cross join docs_df with expanded queries_df\n",
    "cross_joined_df = corpus_tfidf.alias(\"corpus\").crossJoin(queries_df_expanded.alias(\"queries\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf083957-e60a-4dad-bf8e-7d9504085036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/14 08:01:53 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "24/10/14 08:01:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "[Stage 64:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|  id|              to_vec|           tokenized|        raw_features|            features|            features|           tokenized|       dot_product|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "| 100|The rewery Nebras...|[The, rewery, Neb...|(262144,[7058,760...|(262144,[7058,760...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "| 529|The rewery Brasse...|[The, rewery, Bra...|(262144,[7606,812...|(262144,[7606,812...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "| 686|The rewery Elysia...|[The, rewery, Ely...|(262144,[7606,187...|(262144,[7606,187...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "| 674|The rewery Big Ti...|[The, rewery, Big...|(262144,[1641,462...|(262144,[1641,462...|(262144,[66208,12...|[very, bitter, be...|1.6752886309867447|\n",
      "| 302|The rewery Jack's...|[The, rewery, Jac...|(262144,[7606,277...|(262144,[7606,277...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "| 510|The rewery Barfer...|[The, rewery, Bar...|(262144,[7606,129...|(262144,[7606,129...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|5023|The rewery Bube's...|[The, rewery, Bub...|(262144,[336,7123...|(262144,[336,7123...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|3849|The rewery Otto's...|[The, rewery, Ott...|(262144,[4629,760...|(262144,[4629,760...|(262144,[66208,12...|[very, bitter, be...|13.885886163576961|\n",
      "|2436|The rewery Main S...|[The, rewery, Mai...|(262144,[7606,134...|(262144,[7606,134...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|3532|The rewery Faultl...|[The, rewery, Fau...|(262144,[7606,277...|(262144,[7606,277...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|4290|The rewery Calder...|[The, rewery, Cal...|(262144,[1828,760...|(262144,[1828,760...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|3815|The rewery Yuengl...|[The, rewery, Yue...|(262144,[3564,462...|(262144,[3564,462...|(262144,[66208,12...|[very, bitter, be...| 5.025865892960234|\n",
      "|3088|The rewery Goose ...|[The, rewery, Goo...|(262144,[7606,277...|(262144,[7606,277...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|4435|The rewery Steamw...|[The, rewery, Ste...|(262144,[287,1603...|(262144,[287,1603...|(262144,[66208,12...|[very, bitter, be...|3.3505772619734895|\n",
      "|4679|The rewery InBev ...|[The, rewery, InB...|(262144,[424,619,...|(262144,[424,619,...|(262144,[66208,12...|[very, bitter, be...|1.6752886309867447|\n",
      "|2409|The rewery Charli...|[The, rewery, Cha...|(262144,[7606,277...|(262144,[7606,277...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|2605|The rewery Asahi ...|[The, rewery, Asa...|(262144,[7606,195...|(262144,[7606,195...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "| 344|The rewery Little...|[The, rewery, Lit...|(262144,[7606,277...|(262144,[7606,277...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|1356|The rewery Bridge...|[The, rewery, Bri...|(262144,[7606,277...|(262144,[7606,277...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|2512|The rewery Bull &...|[The, rewery, Bul...|(262144,[7606,277...|(262144,[7606,277...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/14 08:02:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Apply the dot product UDF\n",
    "result_df = cross_joined_df.withColumn(\n",
    "    \"dot_product\",\n",
    "    dot_product_udf(cross_joined_df[\"corpus.features\"], cross_joined_df[\"queries.features\"])\n",
    ").cache()\n",
    "\n",
    "# Show result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "42dc8a6c-fc5b-411a-a886-81e11c1fabd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/14 08:02:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|  id|              to_vec|           tokenized|        raw_features|            features|            features|           tokenized|       dot_product|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|5518|The rewery Sabmil...|[The, rewery, Sab...|(262144,[7058,760...|(262144,[7058,760...|(262144,[66208,12...|[very, bitter, be...|105.79780515834159|\n",
      "|3602|The rewery Boston...|[The, rewery, Bos...|(262144,[336,1578...|(262144,[336,1578...|(262144,[16704,27...|[fruity, sour, -,...| 83.91526396115003|\n",
      "|5171|The rewery Roy Pi...|[The, rewery, Roy...|(262144,[991,1581...|(262144,[991,1581...|(262144,[66208,12...|[very, bitter, be...| 78.46171430817284|\n",
      "| 439|The rewery New Be...|[The, rewery, New...|(262144,[535,3456...|(262144,[535,3456...|(262144,[16704,27...|[fruity, sour, -,...|  76.8548726479598|\n",
      "|4282|The rewery Allaga...|[The, rewery, All...|(262144,[303,336,...|(262144,[303,336,...|(262144,[66208,12...|[very, bitter, be...| 64.40356492492458|\n",
      "|4448|The rewery Storm ...|[The, rewery, Sto...|(262144,[1828,365...|(262144,[1828,365...|(262144,[56718,12...|       [weird, beer]| 63.85202462101636|\n",
      "|5733|The rewery Saint ...|[The, rewery, Sai...|(262144,[4629,760...|(262144,[4629,760...|(262144,[66208,12...|[very, bitter, be...| 58.72209867791031|\n",
      "|4422|The rewery Brouwe...|[The, rewery, Bro...|(262144,[7606,114...|(262144,[7606,114...|(262144,[16704,27...|[fruity, sour, -,...| 58.52173674028139|\n",
      "| 481|The rewery Brouwe...|[The, rewery, Bro...|(262144,[1575,219...|(262144,[1575,219...|(262144,[16704,27...|[fruity, sour, -,...| 58.49067566411356|\n",
      "|4676|The rewery The Ch...|[The, rewery, The...|(262144,[4629,760...|(262144,[4629,760...|(262144,[66208,12...|[very, bitter, be...| 54.35183313900411|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.sort(\"dot_product\", ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e84e9-7e74-4fed-8e04-707fb66542de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
