{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4184713b-3d12-4e5b-abc8-d9b85922f01e",
   "metadata": {},
   "source": [
    "# Intro to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca111e-9330-43db-8b1a-d13712013a2f",
   "metadata": {},
   "source": [
    "**Disclaimer:** TP plus compliqué !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806be423-194a-4a2e-8403-2435353e5c3e",
   "metadata": {},
   "source": [
    "**Spark** est un système de calcul hautement parallélisé :\n",
    "- au niveau du stockage : la data est fragmentée, dupliquée, répartie sur un nombre quelconque de disques/servers\n",
    "- au niveau du calcul : les calculs s'exécutent en parallèle sur plusieurs machines, chacune sur sa portion de data\n",
    "\n",
    "Spark en tant que tel est un framework qui a des implémentation dans plusieurs langages :\n",
    "- Python via PySpark (ce qe nous utiliserons)\n",
    "- Scala\n",
    "- R\n",
    "- Java\n",
    "\n",
    "## Quelques informations en vrac sur Spark\n",
    "### Hardware\n",
    "- Spark peut s'installer sur un grand nombre de machines situées dans un même réseau. Elles pourront ensuite se reconnaître et collaborer. 1 unité de calcul = 1 noeud.\n",
    "- Spark fonctionne sur le mode master/worker : un noeud est désigné `master` et jouera le rôle de chef d'orchestre pour que les autres noeuds `workers` exécutent les tâches dans le bon ordre\n",
    "- Les noeuds Spark communiquent énormément entre eux pour s'échanger des informations et surtout des données\n",
    "\n",
    "### Software\n",
    "- Un code Spark / PySpark doit utiliser les primitives Spark pour que tout s'exécute selon la logique Spark\n",
    "- Le code pyspark est transmis au noeud `master` qui le lit et prépare l'orchestration des calculs selon les `workers` qu'il a à disposition. Seuls les `workers` manipuleront la donnée (sauf exception)\n",
    "- Spark est *lazy* : `master` ne lance réellement aucun calcul tant qu'il n'a pas lu d'opération impliquant l'affichage ou l'écriture des résultats\n",
    "- Corrolaire du *lazy* : sans précaution, Spark peut répéter plusieurs fois les mêmes calculs ... Exemple avec 2 chaînes de transformation data `A -> B -> C -> D` suivi de `A -> B -> E`. Les étapes intermédiaires `A -> B` sont identiques mais pour calculer `D` et `E`, Spark risque de les exécuter 2 fois. Apprendre à manipuler les méthodes [cache](https://spark.apache.org/docs/3.5.3/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.cache.html?highlight=cache) !\n",
    "\n",
    "### Framework\n",
    "- Spark se base sur des `DataFrame` très proches en terme d'utilisation des `pandas.DataFrame` donc pas de panique :)\n",
    "- Spark gère tout ce que sait gérer SQL mais part de fichiers plats (ici CSV) : join, select, etc ...\n",
    "- PySpark permet de gérer tout ces concepts avec la facilité d'accès du Python\n",
    "- PySpark est TRÈS typé et a besoin de connaître les types de chaque colonne manipulées\n",
    "- Spark est TRÈS flexible en terme de configuration et d'exécution et cela peut sembler déroutant pour des exemples simples\n",
    "\n",
    "### Organisation du calcul\n",
    "En informatique, à chaque architecture ses optimisations :\n",
    "- en local sur 1 CPU, un calcul possède peu d'optimisation : simple et linéaire, possiblement async\n",
    "- en local sur multi CPU, un calcul doit être prévu pour paralléliser les exécutions sur plusieurs coeurs physiques\n",
    "- en local sur mono/multi GPU, un calcul doit être hautement parallélisable, découpable en tranche de data qui tiennent en GPU-RAM\n",
    "- en multi machine multi GPU, idem que plus haut avec bande passante importante entre machine pour échange d'information\n",
    "\n",
    "... Spark gère le multi machine, multi CPU, multi RAM, multi disque : calculs hautement parallélisés grâce à la magie de Spark, data échangées au mieux entre machine (possiblement avec l'aide humaine).\n",
    "__Spark est toujours prêt à gérer un contexte d'exécution très complexe__ => il faut s'attendre à beaucoup d'overhead sur des cas simples\n",
    "\n",
    "**Les opérations s'exécutent sur des workers séparés, en parallèle**, il faut donc parfois faire \"un peu attention\" à la façon dont on demande à Spark de *partitionner* sa data.\n",
    "\n",
    "## À retenir\n",
    "\n",
    "1. Spark et son implémentation PySpark sont très puissant car gèrent un parallélisme quasi infini et réglable à 100%\n",
    "2. PySpark a un coût d'entrée pour se couler dans le moule Spark mais permet de réaliser des opérations très complexes avec la simplicité du Python\n",
    "3. Votre notebook n'exécutera n'a pas accès à la data manipulée et n'effectuera aucun calculs ; il les transmettra au Spark Master qui les répartira entre ses workers qui ont accès à la data\n",
    "\n",
    "## Exemple de code Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c8621c-611b-4c65-ba9c-521bde327143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f47e72be-8c66-465d-bdbd-13a50d34c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_embedding(text: str):\n",
    "    url = \"http://embedder:8000/embed\"\n",
    "    payload = {\"text\": text}\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f318386f-71ef-44ec-aaf9-160ecd6ff0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c36207d6-6c1d-4e0a-a9ad-5748352ab552",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_ID = \"François\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a7ea6256-c381-4264-99b8-8987b21e8062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/12 14:52:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Initialize PySpark session\n",
    "assert MY_ID is not None, \"provide your id first!\"\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(MY_ID) \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate() \n",
    "\n",
    "# /!\\ Tout se fera à partir de cet object magique `spark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d4fccced-12e3-4a36-a9d4-7884eab014ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     Name|Value|\n",
      "+---------+-----+\n",
      "|    Alice|    1|\n",
      "|      Bob|    2|\n",
      "|Catherine|    3|\n",
      "+---------+-----+\n",
      "\n",
      "CPU times: user 0 ns, sys: 8.07 ms, total: 8.07 ms\n",
      "Wall time: 1.16 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Catherine\", 3)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Value\"])\n",
    "\n",
    "# Show the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0328d481-2517-43d6-af84-935714cc9197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, FloatType, ArrayType, StructField, StructType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "365084ed-4817-426d-919d-13f827892be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+------+--------+---+---+---+---+--------+--------------------+--------+-------------------+\n",
      "| id|brewery_id|                name|cat_id|style_id|abv|ibu|srm|upc|filepath|            descript|add_user|           last_mod|\n",
      "+---+----------+--------------------+------+--------+---+---+---+---+--------+--------------------+--------+-------------------+\n",
      "|  1|       812|         Hocus Pocus|    11|     116|4.5|  0|  0|  0|    NULL|Our take on a cla...|       0|2010-07-22 20:00:20|\n",
      "|  2|       264|   Grimbergen Blonde|    -1|      -1|6.7|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "|  3|       779|Widdershins Barle...|    -1|      -1|9.1|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "|  4|       287|             Lucifer|    -1|      -1|8.5|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "|  5|      1056|              Bitter|    -1|      -1|  4|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "|  6|      1385|       Winter Warmer|     1|      13|5.2|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "|  7|      1099|Winter Welcome 20...|    -1|      -1|  6|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "|  8|      1099|       Oatmeal Stout|     3|      42|  5|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "|  9|       501|     Espresso Porter|    -1|      -1|  0|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "| 10|       545|     Chocolate Stout|     3|      42|  0|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "| 11|       742|Hitachino Nest Re...|    -1|      -1|  7|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "| 12|       779|         JuJu Ginger|    -1|      -1|  4|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "| 13|       545|      The Kidd Lager|     7|      86|  0|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "| 14|      1099|      Imperial Stout|     3|      42|  7|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "| 15|      1315|Oak-Aged Belgian ...|    -1|      -1|  9|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "| 16|       168|         Ultrablonde|    -1|      -1|  8|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "| 17|      1023|  Wiesen Edel Weisse|     4|      50|6.2|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "| 18|        39|    Old Foghorn 2001|    -1|      -1| 10|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "| 19|       196|           Framboise|    -1|      -1|  6|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "| 20|       483|Cow Palace Scotch...|     1|      15|8.7|  0|  0|  0|    NULL|                NULL|       0|2010-07-22 20:00:20|\n",
      "+---+----------+--------------------+------+--------+---+---+---+---+--------+--------------------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_beers = spark.read.csv(\"/datasets/csv/beers.csv\", header=True)\n",
    "df_beers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5194190e-50b3-4e2d-a44c-24ded27a9e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                name| reverse_capitalized|\n",
      "+--------------------+--------------------+\n",
      "|         Hocus Pocus|         SUCOP SUCOH|\n",
      "|   Grimbergen Blonde|   EDNOLB NEGREBMIRG|\n",
      "|Widdershins Barle...|ENIWYELRAB SNIHSR...|\n",
      "|             Lucifer|             REFICUL|\n",
      "|              Bitter|              RETTIB|\n",
      "|       Winter Warmer|       REMRAW RETNIW|\n",
      "|Winter Welcome 20...|8002-7002 EMOCLEW...|\n",
      "|       Oatmeal Stout|       TUOTS LAEMTAO|\n",
      "|     Espresso Porter|     RETROP OSSERPSE|\n",
      "|     Chocolate Stout|     TUOTS ETALOCOHC|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "CPU times: user 8.17 ms, sys: 375 μs, total: 8.54 ms\n",
      "Wall time: 1.14 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Define a specific funtion to map beer names\n",
    "@F.udf(returnType=StringType())\n",
    "def revert_cap_name(name: str):\n",
    "    return name[::-1].upper()\n",
    "\n",
    "\n",
    "df_beers.limit(10).withColumn(\"reverse_capitalized\", revert_cap_name(F.col(\"name\"))).select(\"name\", \"reverse_capitalized\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d25580a-421d-41c6-bdb8-6ebe81a4b39b",
   "metadata": {},
   "source": [
    "## Observations\n",
    "Que remarque-t-on tout de suite ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9672354a-2827-42fc-a265-fcedd0e8affc",
   "metadata": {},
   "source": [
    "# Uses cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19828ca-a8ba-4727-8258-b748ce8afb83",
   "metadata": {},
   "source": [
    "# UC-1 : description data\n",
    "\n",
    "- Q1: Combien y a-t-il de bières dans la DB ?\n",
    "- Q2: Top10 brasseries les plus représentées avec le nombre de bière par brasserie ?\n",
    "- Q3: Top10 des bières les plus fortes (ABV) en France ?\n",
    "- Q4: Par pays, nombre de brasseries qui proposent des bières de type `Porter` et ABV moyen de celles-ci ?\n",
    "- Q5: Mediane du nombre de bière par pays ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b2d29ccd-27f6-4235-a3c1-996c14480f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.33 ms, sys: 0 ns, total: 3.33 ms\n",
      "Wall time: 508 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7059"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Q1\n",
    "df_beers = spark.read.csv(\"/datasets/csv/beers.csv\", header=True)\n",
    "df_beers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d737d5af-cf1d-4623-b7fd-5b94732bd6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7059"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_beers = (\n",
    "    df_beers\n",
    "    .withColumnRenamed(\"abv\", \"abv_old\")\n",
    "    .withColumn(\"abv\", F.col(\"abv_old\").cast(FloatType()))\n",
    "    .drop(\"abv_old\")\n",
    ")\n",
    "df_beers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "099d552b-7cd7-436f-806f-cf4321ad876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brew = spark.read.csv(\"/datasets/csv/breweries.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6cccbce5-7f70-4624-acb3-86a8768bb6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----+\n",
      "|  id|                name|count|\n",
      "+----+--------------------+-----+\n",
      "| 858|Midnight Sun Brew...|   57|\n",
      "|1072|          Rogue Ales|   49|\n",
      "|  44|      Anheuser-Busch|   38|\n",
      "| 483|        Egan Brewing|   37|\n",
      "|1286|      Troegs Brewing|   37|\n",
      "| 157| Boston Beer Company|   36|\n",
      "|1268|   Titletown Brewing|   34|\n",
      "| 512|   F.X. Matt Brewing|   34|\n",
      "|1142|Sierra Nevada Bre...|   33|\n",
      "|1204|   Stone Brewing Co.|   32|\n",
      "|1173|Southern Tier Bre...|   31|\n",
      "|1352|Weyerbacher Brewi...|   30|\n",
      "| 590|   Gottberg Brew Pub|   28|\n",
      "| 585|Goose Island Beer...|   28|\n",
      "|  45|Appalachian Brewi...|   27|\n",
      "| 459|Dogfish Head Craf...|   27|\n",
      "|1326|     Victory Brewing|   25|\n",
      "| 100| Bell's Brewery Inc.|   25|\n",
      "| 441|   Deschutes Brewery|   22|\n",
      "| 960|Otto's Pub and Br...|   21|\n",
      "+----+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 6.08 ms, sys: 173 μs, total: 6.26 ms\n",
      "Wall time: 936 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Q2 \n",
    "df_brew = spark.read.csv(\"/datasets/csv/breweries.csv\", header=True)\n",
    "df = (\n",
    "    df_beers\n",
    "    .join(df_brew, on=df_beers.brewery_id==df_brew.id)\n",
    "    .groupby([df_brew.id, df_brew.name])\n",
    "    .count()\n",
    "    .sort(F.col(\"count\").desc())\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4a1be919-2311-4bb6-adea-6b6607777b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5906"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3 top 10 bières FR les plus fortes\n",
    "df_beers_and_brew = df_beers.join(df_brew, on=df_beers.brewery_id==df_brew.id).cache()\n",
    "df_beers_and_brew.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "44b65e7d-ea7f-4506-b1cf-0dc7929436a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|                name| abv|\n",
      "+--------------------+----+\n",
      "|           Belzebuth|13.0|\n",
      "|Gavroche French R...| 8.5|\n",
      "|             3 Monts| 8.5|\n",
      "|                Yeti| 8.0|\n",
      "|      Jenlain Blonde| 7.5|\n",
      "|              Blonde| 7.5|\n",
      "|   Les Sans Culottes| 7.0|\n",
      "|           Framboise| 6.0|\n",
      "|Jenlain St Druon ...| 6.0|\n",
      "|Castelain St.Aman...| 5.9|\n",
      "|Castelain Blond B...| 5.6|\n",
      "|                XTra| 4.5|\n",
      "|                Jade| 4.5|\n",
      "|     BiÃ¨re de NoÃ«l| 0.0|\n",
      "|AmbrÃ©e / Chestnu...| 0.0|\n",
      "|    BÃªte des Vosges| 0.0|\n",
      "|                1664| 0.0|\n",
      "|                Ambr| 0.0|\n",
      "+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    df_beers_and_brew\n",
    "    .filter(df_beers_and_brew.country==F.lit(\"France\"))\n",
    "    .sort(F.col(\"abv\").desc())\n",
    "    .select([df_beers.name, \"abv\"])\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ea77c90d-e98c-41db-a489-a17b6baff6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>style_name</th>\n",
       "      <th>last_mod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>Robust Porter</td>\n",
       "      <td>2010-06-15 19:17:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>Brown Porter</td>\n",
       "      <td>2010-06-15 19:17:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>Porter</td>\n",
       "      <td>2010-06-15 19:21:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>Smoke Porter</td>\n",
       "      <td>2010-06-15 19:24:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>American-Style Imperial Porter</td>\n",
       "      <td>2010-06-15 19:25:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>Porter</td>\n",
       "      <td>2010-06-15 19:25:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>106</td>\n",
       "      <td>9</td>\n",
       "      <td>Baltic-Style Porter</td>\n",
       "      <td>2010-06-15 19:42:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id cat_id                      style_name             last_mod\n",
       "17    18      1                   Robust Porter  2010-06-15 19:17:37\n",
       "18    19      1                    Brown Porter  2010-06-15 19:17:43\n",
       "24    25      2                          Porter  2010-06-15 19:21:39\n",
       "38    39      3                    Smoke Porter  2010-06-15 19:24:30\n",
       "44    45      3  American-Style Imperial Porter  2010-06-15 19:25:33\n",
       "45    46      3                          Porter  2010-06-15 19:25:53\n",
       "105  106      9             Baltic-Style Porter  2010-06-15 19:42:40"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a.style_name.apply(lambda s: \"porter\" in s.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6103e61c-7965-42f3-bf38-d78d7b6874d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4\n",
      "+--------------+------------------+----------------------+\n",
      "|       country|           avg_abv|n_brewer_having_porter|\n",
      "+--------------+------------------+----------------------+\n",
      "| United States|2.3305555541291194|                   216|\n",
      "|United Kingdom|3.9500000136239186|                    13|\n",
      "|        Canada|               0.0|                     5|\n",
      "|        Russia|               7.0|                     1|\n",
      "|        Sweden|               5.5|                     1|\n",
      "|       Germany| 7.099999904632568|                     1|\n",
      "|     Lithuania| 6.800000190734863|                     1|\n",
      "|        Norway|               7.0|                     1|\n",
      "|       Denmark|               8.0|                     1|\n",
      "|   Switzerland|               4.5|                     1|\n",
      "|Czech Republic|               8.0|                     1|\n",
      "|         Japan|               0.0|                     1|\n",
      "|        Poland| 8.300000190734863|                     1|\n",
      "|     Australia|               0.0|                     1|\n",
      "+--------------+------------------+----------------------+\n",
      "\n",
      "CPU times: user 9.19 ms, sys: 473 μs, total: 9.66 ms\n",
      "Wall time: 854 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Q4\")\n",
    "df_style = spark.read.csv(\"/datasets/csv/styles.csv\", header=True)\n",
    "target_style_id = (\n",
    "    df_style\n",
    "    .filter(F.col(\"style_name\").ilike(\"porter\"))\n",
    "    .select(F.col(\"id\").alias(\"style_id\"))\n",
    ")\n",
    "dd = (\n",
    "    df_beers_and_brew\n",
    "    .join(target_style_id, how=\"inner\", on=\"style_id\")\n",
    "    .select([df_brew.name, df_brew.name.alias(\"brewer_name\"), \"abv\", \"country\"])\n",
    "    .groupby(\"country\")\n",
    "    .agg(F.avg(\"abv\").alias(\"avg_abv\"), F.countDistinct(\"brewer_name\").alias(\"n_brewer_having_porter\"))\n",
    "    .sort(F.col(\"n_brewer_having_porter\").desc())\n",
    ")\n",
    "dd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f4fb54fa-6c56-4201-8918-125fe4eca640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5\n",
      "+--------------------+-----+\n",
      "|             country|count|\n",
      "+--------------------+-----+\n",
      "|              Russia|    6|\n",
      "|Macedonia, the Fo...|    1|\n",
      "|               Macao|    1|\n",
      "|              Sweden|    7|\n",
      "|         Philippines|    2|\n",
      "|             Germany|  302|\n",
      "|              France|   18|\n",
      "|              Greece|    2|\n",
      "|           Sri Lanka|    2|\n",
      "|                Togo|    1|\n",
      "|            Slovakia|    1|\n",
      "|           Argentina|    4|\n",
      "|             Belgium|  331|\n",
      "|             Finland|    3|\n",
      "|             Myanmar|    1|\n",
      "|        Sierra Leone|    1|\n",
      "|       United States| 4552|\n",
      "|               India|   15|\n",
      "|               China|    2|\n",
      "|             Croatia|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 3.7 ms, sys: 0 ns, total: 3.7 ms\n",
      "Wall time: 214 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Q5\")\n",
    "dd = (\n",
    "    df_beers_and_brew\n",
    "    .groupby(\"country\")\n",
    "    .count()\n",
    "    #.agg(F.median(\"count\"))\n",
    ")\n",
    "#print(\"Q5:\", dd.first()[0])\n",
    "dd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7164ee65-9d0b-43bd-9532-6b97e855399d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Russia</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Macedonia, the Former Yugoslav Republic of</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Macao</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Philippines</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Hungary</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Mauritius</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Korea, Republic of</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       country  count\n",
       "0                                       Russia      6\n",
       "1   Macedonia, the Former Yugoslav Republic of      1\n",
       "2                                        Macao      1\n",
       "3                                       Sweden      7\n",
       "4                                  Philippines      2\n",
       "..                                         ...    ...\n",
       "56                                     Hungary      4\n",
       "57                                   Mauritius      1\n",
       "58                              United Kingdom    210\n",
       "59                          Korea, Republic of      3\n",
       "60                                 Netherlands     29\n",
       "\n",
       "[61 rows x 2 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab51b61-74aa-4a29-84fd-99f87d8bf5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e82d2e7-274a-49e8-bb1f-5d491bec2609",
   "metadata": {},
   "source": [
    "# UC-2 : préparer un dataset de ranking \n",
    "Tout moteur de recherche/search-engine - **SE** - nécessite de la configuration ... beaucoup de configuration. Une des configuration très orientée \"data\" est le calcul que l'index doit opérer pour scorer chaque réponse possible face à une requête. L'apprentissage statistique de ce score s'appelle *Learning to Rank*  - **LTR** - et nécessite des connaissances poussées en machine learning. \n",
    "\n",
    "Cette tâche LTR se base sur les *feedbacks implicites* des utilisateurs face au moteur de recherche. Commençons par un exemple. Quand vous cherchez un objet sur LeBonCoin, vous laissez plusieurs informations *implicites* sur votre perception des résultats proposés : les item sur lesquels vous avez cliqués bien sûr mais également ceux que vous avez probablement *vu* sans cliquer dessus ... Ces \"vues sans clics\" sont une précieuse information implicite sur les jugement que vous avez porté aux résultats proposés. Pour ce TP nous nous limiterons à ce concept de \"vu x click\" mais il est possible d'aller plus loin (dwell-time, hierarchisation des interactions explicites, ...). \n",
    "\n",
    "On appelle *Search Engine Results Page* - **SERP** - la liste des résultats classés par un SE. Un document qui figure dans les résulats d'une recherche a donc une position (son rang) au sein de la **SERP**.\n",
    "\n",
    "Exemple, où :\n",
    "- `query` est la recherche réalisée par un user et qui a débouché sur une SERP\n",
    "- `clicked_id` : l'id de la bière cliquée par le user\n",
    "- `user_id` l'id de l'utilisateur (simplifions en disant que c'est même l'id d'une recherche) : permet de retrouver tous les résultats proposés dans **une** recherche\n",
    "- `id_in_serp` : l'id d'une bière figurant dans la SERP\n",
    "- `pos_in_serp` : la position/le rang de la bière `id_in_serp` dans la SERP issue de la recherche "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2153f482-d334-4f46-8134-d25dfe0643a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+----------+-----------+\n",
      "|      query|clicked_id|             user_id|id_in_serp|pos_in_serp|\n",
      "+-----------+----------+--------------------+----------+-----------+\n",
      "|fruity sour|      4442|ecfce536-7fc5-11e...|      4442|          1|\n",
      "|fruity sour|      4442|ecfce536-7fc5-11e...|       475|          2|\n",
      "|fruity sour|      4442|ecfce536-7fc5-11e...|       481|          3|\n",
      "+-----------+----------+--------------------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_pref = spark.read.csv(\"/datasets/beers_feedback.csv\", header=True, inferSchema=True)\n",
    "df_pref.limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28882f0-de9a-4fbc-97c5-a37864103477",
   "metadata": {},
   "source": [
    "Un travail préliminaire au LTR est la constitution d'un dataset qui permet d'aggréger ces feedbacks laissés par tous les utilisateurs ayant réalisé la même query. Chacun a vu et cliqué selon ses propres impressions de pertinence et il convient de \"moyenner\" tout cela pour obtenir des appréciations globales. L'objectif d'un tel dataset est de pouvoir lister des exemples de triplets `(query, document, note)` qui permet de savoir que face à une *query* `milky stout low bitterness`, un *document* `Super bitter beer brewed with organic roasted barley and chocolate` aura une pertinence de *1/4* (arbitraire). \n",
    "\n",
    "Implémenter le modèle d'agrégation de feedback \"cascade model\" [1] (pour la culture, **inutile d'avoir lu l'article** pour le TD) qui propose une approche pragmatique pour obtenir ces données. La méthode est la suivante :\n",
    "- pour chaque recherche utilisateur:\n",
    "    - étudier la position de l'id cliqué dans la SERP - soit `clicked_pos_in_serp` cette information\n",
    "    - Considérer que tout doc situés \"au-dessus dans la SERP\" (càd quand `pos_in_serp <= clicked_pos_in_serp`) avait été vu par l'utilisateur\n",
    "    - Récapituler tous ces documents \"vus et cliqués\" et \"vus mais pas cliqués\"\n",
    "- Pour chaque recherche et bière cliquée (`clicked_id`), calculer la \"probabilité de clic sachant qu'elle a été vue\", càd le nombre de fois qu'elle a été cliquée divisé par le nombre de fois où elle a été vue\n",
    "\n",
    "\n",
    "[1] https://dl.acm.org/doi/abs/10.1145/1341531.1341545"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4564c432-28ad-4699-b65d-f4f524dd14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output schema : https://stackoverflow.com/a/54771215/10716281\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "mapping = {\n",
    "    \"float64\": DoubleType,\n",
    "    \"object\":StringType,\n",
    "    \"int64\":IntegerType,\n",
    "    \"int32\":IntegerType,\n",
    "    \"bool\": BooleanType,\n",
    "} # Incomplete - extend with your types.\n",
    "\n",
    "def createUDFSchemaFromPandas(dfp):\n",
    "  column_types  = [StructField(key, mapping[str(dfp.dtypes[key])]()) for key in dfp.columns]\n",
    "  schema = StructType(column_types)\n",
    "  return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c2fc2e9b-3d4c-4b02-b93f-d2401b1b34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cascade_model_per_user_query(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    pos_of_clicked_id = df[df[\"id_in_serp\"] == df[\"clicked_id\"]].iloc[0][\"pos_in_serp\"]\n",
    "    df[\"seen\"] = (df[\"pos_in_serp\"] <= pos_of_clicked_id).astype(int)\n",
    "    df[\"clicked\"] = np.where(df[\"pos_in_serp\"] == pos_of_clicked_id, 1, 0)\n",
    "    return df\n",
    "\n",
    "df_processed = compute_cascade_model_per_user_query(df_pref.limit(3).toPandas())\n",
    "schema = createUDFSchemaFromPandas(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b0ea38c6-4080-4530-a238-17f93c3e780f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:==========================================>             (12 + 4) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------------------+------+\n",
      "|               query|id_in_serp|         clic_proba|n_seen|\n",
      "+--------------------+----------+-------------------+------+\n",
      "|high ABV imperial...|      5778|                0.6|    10|\n",
      "|amber ale with ca...|      4945|                0.6|    10|\n",
      "|chocolate stout w...|        10|                0.6|    10|\n",
      "|non-alcoholic chr...|      2216|                0.5|    12|\n",
      "|citrusy West Coas...|      5804|                0.5|    10|\n",
      "|strong sour with ...|      4442|                0.5|    10|\n",
      "|tropical flavored...|      2922|0.46153846153846156|    13|\n",
      "|     malty amber ale|      3740|0.45454545454545453|    11|\n",
      "|         fruity sour|       481| 0.4166666666666667|    12|\n",
      "|           dry stout|       345|                0.4|    10|\n",
      "|    malty scotch ale|      5783|0.35714285714285715|    14|\n",
      "|     smoky rauchbier|      3364| 0.3333333333333333|    15|\n",
      "|         milky stout|      2001| 0.3333333333333333|    12|\n",
      "|         bitter sour|      4442| 0.3333333333333333|    12|\n",
      "|           light IPA|      4506| 0.3076923076923077|    13|\n",
      "|     low alcool pils|      1641| 0.3076923076923077|    13|\n",
      "|    smooth brown ale|      4996|                0.3|    10|\n",
      "|   pale ale under 5%|      1617|                0.3|    10|\n",
      "|high abv belgian ...|      4910|                0.3|    10|\n",
      "|barley wine with ...|      5409|                0.3|    10|\n",
      "+--------------------+----------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "dd = (\n",
    "    df_pref\n",
    "    .repartition(16, \"query\", \"user_id\")\n",
    "    .groupby([\"query\", \"user_id\"])\n",
    "    .applyInPandas(compute_cascade_model_per_user_query, schema)\n",
    "    #.filter(F.col(\"seen\") == F.lit(1))\n",
    "    .groupby([\"query\", \"id_in_serp\"])\n",
    "    .agg(F.sum(\"seen\").alias(\"n_seen\"), F.sum(\"clicked\").alias(\"n_clicked\"))\n",
    "    .withColumn(\"clic_proba\", F.col(\"n_clicked\") / F.col(\"n_seen\"))\n",
    "    .filter(F.col(\"n_seen\") >= F.lit(10))\n",
    "    .sort(F.col(\"clic_proba\").desc())\n",
    "    .select([\"query\", \"id_in_serp\", \"clic_proba\", \"n_seen\"])\n",
    ")\n",
    "dd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b7b8484e-a14a-4f8d-b416-524cddfa3091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff64cf14",
   "metadata": {},
   "source": [
    "# UC-3 : récupérer les docs qui parlent d'un mot\n",
    "\n",
    "Peut-on utiliser SQL pour réaliser un mini moteur de recherche ? Pour différentes requêtes (`query` en anglais) textuelles très simples à base de mot-clef, retrouver les bières qui semblent répondre à la demande. Exemples :\n",
    "- trouver les bières ou les brasseries qui parlent de bières \"fine\"\n",
    "- idem pour \"juicy\"\n",
    "- idem pour \"genuine\"\n",
    "- idem pour les bières mâturées dans des \"oak cask\" (fûts en chêne) -> combien y en a-t-il ? $N_1$\n",
    "   - idem pour les bières qui évoquent uniquement \"cask\" -> combien y en a-t-il ? $N_{1,1}$\n",
    "   - idem pour celles ne parlant que de \"oak\" -> combien y en a-t-il ? $N_{1,2}$\n",
    "- idem pour les bières qui évoquent \"oak\" et \"cask\" -> combien y en a-t-il ? $N_{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ba5574-b2b3-41c2-af30-42fe45bd2e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9fb62b-e0de-4828-bf80-41cd8b24a1cc",
   "metadata": {},
   "source": [
    "## UC-3.1 langdetect over descripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "61eb6d83-4f06-4fbc-976b-6c1bc4eee1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def detect_lang(text):\n",
    "    try:\n",
    "        return langdetect.detect(text)\n",
    "    except langdetect.LangDetectException:\n",
    "        return \"ukn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b287623c-8ad2-487e-8cdd-74822b014bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=StringType())\n",
    "def agg_descriptions(beer_descr, brewer_descr):\n",
    "    descr = \"\"\n",
    "    if beer_descr:\n",
    "        descr += \" \" + beer_descr\n",
    "    if brewer_descr:\n",
    "        descr += \" \" + brewer_descr\n",
    "    return descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "557414cd-c522-40ac-85e0-7dd5c7672ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 86:=============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|lang|count|\n",
      "+----+-----+\n",
      "|  en| 2354|\n",
      "|  vi|    4|\n",
      "|  de|    6|\n",
      "|  es|    1|\n",
      "|  af|   62|\n",
      "| ukn| 3464|\n",
      "|  fr|    3|\n",
      "|  tl|    3|\n",
      "|  nl|    2|\n",
      "|  cy|    1|\n",
      "|  ro|    1|\n",
      "|  hu|    1|\n",
      "|  ca|    2|\n",
      "|  it|    1|\n",
      "|  da|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "dd = (\n",
    "    df_beers.withColumnRenamed(\"descript\", \"beer_descr\")\n",
    "    .join(\n",
    "        df_brew.withColumnRenamed(\"descript\", \"brewer_descr\"),\n",
    "        on=df_beers.brewery_id==df_brew.id\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"descr\", \n",
    "        agg_descriptions(\n",
    "            F.coalesce(F.col(\"beer_descr\"), F.lit(\"\")), \n",
    "            F.coalesce(F.col(\"brewer_descr\"), F.lit(\"\"))\n",
    "        )\n",
    "    )\n",
    "    .repartition(4)\n",
    "    .withColumn(\"lang\", detect_lang(F.col(\"descr\")))\n",
    "    .groupby(F.col(\"lang\")).count()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675fd4dd-8135-4f27-a0cb-2c8baeb1df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "%%time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d942b-9c0d-46b5-afa1-b46dfcb4393f",
   "metadata": {},
   "source": [
    "# UC-4 : vectorisation des description des bières\n",
    "Préparer le recours à un service de vectorisation qui permettra de convertir la connaissance sur une bière en un vecteur numérique. Ce vecteur permet de sythétiser mathématiquement l'information disponible sur une bière et sa brasserie et pourra être réutilisé plus tard dans un moteur de recherche.\n",
    "à faire :\n",
    "- Préparer une description la plus complète possible pour chaque bière\n",
    "- envoyer ces descriptions une à une via un appel HTTP sur `vectorizer` (voir instruction plus bas)\n",
    "\n",
    "\n",
    "## Service de vectorisation\n",
    "Nous allons faire appel à un service de vectorisation dans le cluster http://vectorizer. Une autre option aurait été d'utiliser un service de vectorisation externe - Jina - qui propose gratuitement 1M token de vectorisation ... mais problème de black listing de l'IP de l'UL.\n",
    "\n",
    "Snippet de code pour l'utiliser :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037d6c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_embedding(text: str):\n",
    "    url = \"http://vectorizer:8000/embed\"\n",
    "    payload = {\"text\": text}\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    try:\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        return response.json()[\"vector\"]\n",
    "    except:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b1cfee-f1dc-4842-bd80-c30f4d1c57b3",
   "metadata": {},
   "source": [
    "Proposer un texte synthétique à vectoriser, puis paralléliser les appels au Vectorizer via Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d106a-bcd8-4d46-9200-c7d6bd88797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee22f37-b07e-4f63-935e-f153e680f089",
   "metadata": {},
   "source": [
    "# UC-5 : answer question in corpa\n",
    "\n",
    "**Question difficile en Spark**\n",
    "\n",
    "**Grandes lignes :** trouvons les documents qui répondent à une question. Exemple : à partir de la description vectorisée à UC-4 pour chaque bière, comment trouver les bières qui répondent à une description plus complète ? Exemple:\n",
    "- \"very bitter beer with smoky taste\"\n",
    "- \"fruity sour - balanced sourness\"\n",
    "- \"weird beer\"\n",
    "\n",
    "Voir la doc [Spark ML lib - feature extraction](https://spark.apache.org/docs/latest/api/python/reference/pyspark.mllib.html#feature) pour trouver des idées (TF-IDF, Word2Vec), ou utiliser le résultats de vos vectorisation de UC-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3257601-6abd-403f-9a13-3f59e2f1973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"very bitter beer with smoky taste\", \"fruity sour - balanced sourness\", \"weird beer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
