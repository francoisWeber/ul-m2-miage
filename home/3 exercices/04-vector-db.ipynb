{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e218368-0e0a-4cdf-accb-698b731c39de",
   "metadata": {},
   "source": [
    "# Index vectoriel : NN et ANN, équivalent ML des index lexicaux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe8a65-bb26-4ad3-8e04-aa108602eaa1",
   "metadata": {},
   "source": [
    "Depuis ~2020, le ML permet de représenter des documents textuels par des vecteurs en grande dimension (appelés **embedding**) qui possèdent l'énorme propriété de traduire numériquement/vectoriellement l'information sémantique contenue dans les documents. De surcroit, ces embeddings peuvent se comparer algébriquement très simplement dans le sens où 2 embeddings \"proches\" (dans leur espace) correspondent à des objets proches (dans notre perception). Le uses-case 4 des TP précédent visait à obtenir de tels embeddings à partir d'un service externe.\n",
    "\n",
    "Des structures de données spécifiques ont été proposées pour tirer profit des avantages techniques des embeddings. On parlera de DB vectorielle (DBV) pour ce type de base de données vectorielles. Ces DBV nécessitent une technologie très différentes des index lexicaux/float traditionnels et implémentent différents algorithmes de résolution des problèmes *Nearest Neighbors* (plus proches voisins) ou plus fréquemment *Approximated Nearest Neighbors* (plus proches voisins approximatifs).\n",
    "## Recherche par plus proche voisins \n",
    "### Algorithme Nearest Neighbors - NN\n",
    "\n",
    "Puisqu'il est possible de représenter (presque) tout document sous format embedding et que ces derniers ont la propriété d'être comparables entre eux, un nouveau type de recherche s'ouvre : recherche par embeddings les plus proche de l'embedding d'une query. Opérer une recherche à partir d'une query revient à trouver les *plus proches voisins* (Nearest Neighbors - NN) de l'embedding de la query parmi les embeddings de documents.\n",
    "\n",
    "Exemple en dimension 2 : les coordonnées GPS 2D d'une ville sont en quelque sort un embedding basique d'une ville. Trouver les 5 villes les plus proches de Nancy est simple : il suffit de cacluler les distances de Nancy à toutes les villes grâce à leurs coordonnées et de trouver les top5 distances les plus faibles.\n",
    "\n",
    "**Problème:** une telle recherche exhaustive implique $O(n^2)$ calculs où $n$ est le nombre de villes. Si $n=10^6$, le calcul devient difficile.\n",
    "\n",
    "### Variante Approximative Nearest Neihbors - ANN\n",
    "\n",
    "**Solution proposée:** sachant qu'il est inutile de calculer la distance entre Nancy et Timbuktu ou New-York (celles-ci ne seront jamais dans le top5 proximité), il peut être intéressant de restreindre le champ de recherche afin de ne payer une recherche exhaustive en $O(n^2)$ que pour une poignée de villes qu'on sait déjà être \"proches\". Dans notre exemple, une recherche limitée au département de la ville cible et aux départements limitrophes suffit. \n",
    "\n",
    "Il s'agit d'un début de compréhension de la famille d'algorithmes Approximative Nearest Neihbors (ANN par la suite) qui permet de casser la complexité du problème de recherche de plus proches voisins en hierarchisant l'information. Cette hierarchisation se fait via une structure de donnée particulière ; l'implémentation la plus courante en 2024 est [Hierarchical Navigable Small World - HNSW](https://www.wikiwand.com/en/articles/HNSW_indexes).\n",
    "\n",
    "Remarque : l'algo qui traduit réellement la hierarchisation stricte est plutôt de la famille [K-d tree](https://www.wikiwand.com/en/articles/K-d_tree) mais il est inefficace pour des vecteurs de dimension $k=768+$ comme c'est le cas pour la plupart des embeddings\n",
    "\n",
    "## Solutions open sources\n",
    "Depuis quelques années, de nouveaux acteurs spécialisés en DB vectorielles ont émergé. Les acteurs historiques de l'indexation s'y mettent:\n",
    "- ElasticSearch : se met \"sur le tard\" aux ANN ([ici](https://www.elastic.co/fr/blog/introducing-approximate-nearest-neighbor-search-in-elasticsearch-8-0)) - ils sont possiblement ralentis par leur coeur-algo Lucent qui me semble peu flexible\n",
    "- Vespa : comme indiqué au TP précédent, Vespa intègre nativement tout l'atirail vectoriel et ANN, de façon distribué (le [blog](https://blog.vespa.ai/index.html) abonde de sujets ANN)\n",
    "- PostgresSQL : la DB SQL historique a démarré un projet [pgvector](https://github.com/pgvector/pgvector/) pour proposer un support ANN\n",
    "- Redis Vector : le caching utilisé en TP peut également être utilisé comme index vectoriel\n",
    "De nouveaux acteurs \"pure players\" des DB vectorielles ont vu le jour également :\n",
    "- FAISS : [lib open-source](https://github.com/facebookresearch/faiss) de Meta. Interface rustique mais grande puissance. Plutôt pour tester que pour mettre en prod\n",
    "- Milvus : d'après leur [site](https://milvus.io/) il s'agit d'une DBV scalabale. En prod chez Le Bon Coin\n",
    "- Qdrant : idem d'après [leur site](https://qdrant.tech/) - utilisée uniquement dans des PoC persos\n",
    "- Weaviate : idem d'après [leur site](https://weaviate.io/)\n",
    "\n",
    "Certains services proposent des DBV managées dans le cloud (ex: Weaviate, Vespa, ...), d'autres proposent simplement le code (ex: FAISS).\n",
    "\n",
    "**Dans ce TP, nous utiliserons rapidement Vespa et surtout Qdrant**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01894494-77de-48de-87d6-352f86b574c2",
   "metadata": {},
   "source": [
    "## Reprise de l'exemple avec Vespa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad74b2ff-2bf8-40e7-a925-c5b73c4f8a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Application is up!\n"
     ]
    }
   ],
   "source": [
    "from vespa.application import Vespa, VespaSync\n",
    "import json\n",
    "\n",
    "vespa = Vespa(url=\"http://vespa\", port=8080)\n",
    "vespa.wait_for_application_up(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "071298a7-6644-4982-ad79-1e4fd8fb7ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": \"id:beer_content:beer::5818\",\n",
      "    \"relevance\": 0.7768163607448544,\n",
      "    \"source\": \"beer_content\",\n",
      "    \"fields\": {\n",
      "      \"sddocname\": \"beer\",\n",
      "      \"documentid\": \"id:beer_content:beer::5818\",\n",
      "      \"name\": \"The Angel's Share - Bourbon Barrel Aged\",\n",
      "      \"description_beer\": \"own in Kentucky and across the pond in Scotland, distillers who age their whiskeys for many years refer to the evaporation of the spirits from their barrels as \\\"The Angel's Share.\\\" We couldn\\u00e2\\u20ac\\u2122t agree more. Each time a barrel is filled, a measure of liquid seeps into the oak and is lost for good.\\r\\n\\r\\nThis striking Strong Ale is brewed with copious amounts of Caramel malt to emphasize the vanilla and oak flavors found in freshly emptied bourbon or brandy barrels. The beer spends a year in oak before it is packaged for release. The beer is 12.5% ABV and is available in 375ml and 750ml bottles and on draft at inspired locations.\",\n",
      "      \"brewery\": \"The Lost Abbey\",\n",
      "      \"country\": \"United States\",\n",
      "      \"cat_name\": \"North American Ale\",\n",
      "      \"style_name\": \"American-Style Strong Pale Ale\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"id:beer_content:beer::4653\",\n",
      "    \"relevance\": 0.772828586688974,\n",
      "    \"source\": \"beer_content\",\n",
      "    \"fields\": {\n",
      "      \"sddocname\": \"beer\",\n",
      "      \"documentid\": \"id:beer_content:beer::4653\",\n",
      "      \"name\": \"Oaked Arrogant Bastard Ale\",\n",
      "      \"description_beer\": \"Arrogant Bastard Ale aged in oak barrels.\",\n",
      "      \"brewery\": \"Stone Brewing Co.\",\n",
      "      \"country\": \"United States\",\n",
      "      \"cat_name\": \"North American Ale\",\n",
      "      \"style_name\": \"American-Style Strong Pale Ale\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"id:beer_content:beer::5857\",\n",
      "    \"relevance\": 0.7670398659002798,\n",
      "    \"source\": \"beer_content\",\n",
      "    \"fields\": {\n",
      "      \"sddocname\": \"beer\",\n",
      "      \"documentid\": \"id:beer_content:beer::5857\",\n",
      "      \"name\": \"Samael's Oak-aged Ale\",\n",
      "      \"description_beer\": \"A super-caramelly, oak-aged English-style strong ale. The oak is very apparent in this rich and high gravity ale. Additional depth and complexity result in a woody and cask-like nose, with a pronounced vanilla flavor on the palate. As of 2007, the use of additional roasted malt has resulted in subtle bitternes to balance the natural sweetness.\",\n",
      "      \"brewery\": \"Avery Brewing Company\",\n",
      "      \"country\": \"United States\",\n",
      "      \"cat_name\": \"British Ale\",\n",
      "      \"style_name\": \"Strong Ale\",\n",
      "      \"description_brewery\": \"Established in 1993, Avery Brewing Co. is a family owned and operated micro-brewery dedicated to brewing the finest quality English and Belgian style ales.. From humble beginnings, brewing 800 barrels of three different beers in 1994, we have progressed to brewing 13,000 barrels of twenty different beers in 2007. We attribute this success to beer drinkers gravitating to beers with more interesting flavor profiles. The unique flavor complexity of Avery beers occurs through a combination of sparing no expense with regard to ingredients and our hopping methods. Sparing no expense means using lots of specialty malts, imported hops, such as Styrian Goldings, and imported Belgian candy sugar.\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "query = \"scottish oak cask beer\"\n",
    "resp = vespa.query(\n",
    "    {\n",
    "        \"yql\": \"select * from beer where {targetHits:3}nearestNeighbor(mrl_embedding, q)\",\n",
    "        \"input.query(q)\": \"embed(mxbai, @text)\",\n",
    "        \"ranking.profile\": \"ann\",\n",
    "        \"text\": query\n",
    "    }\n",
    ")\n",
    "print(json.dumps(resp.json[\"root\"][\"children\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293d4587-b80f-43b0-ab4e-61b028bdd8f0",
   "metadata": {},
   "source": [
    "# Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ffad4-9971-4866-8478-1efa58e6ad3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
