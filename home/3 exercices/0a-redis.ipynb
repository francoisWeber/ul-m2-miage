{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b338aa-874a-49ca-b2a0-991a8385b1e1",
   "metadata": {},
   "source": [
    "# Intro au caching - Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b588b-2004-4581-bfef-aae4b9dbc569",
   "metadata": {},
   "source": [
    "**Exos sans recours aux use-cases 1-5**\n",
    "\n",
    "Les caches sont des instances qui permettent de retenir ponctuellement des paires clef/valeur lorsque l'obtention des valeurs est couteuse/longue. Voyons les caches comme des dictionnaires / hashmap qui permettent une récupération très rapides de ces valeur indexées par une clef. Le stockage du dictionnaire se fait souvent en RAM pour accélérer l'insertion et la récupération.\n",
    "\n",
    "## Principe de fonctionnement\n",
    "\n",
    "Un cache est un intermédiaire entre 2 services : un `client` et un `serveur`. Plutôt que d'interroger directement le `serveur` pour obtenir une réponse à propos d'un objet (quelconque) `A`, le `client` va interroger le cache pour savoir s'il connait une clef qui correspond à `A` : `KEY:A`. Alors :\n",
    "- si le cache ne connaît pas la clef `KEY:A`, alors\n",
    "  - le `serveur` est tout de même interroger et renvoie la réponse `RESP(A)`\n",
    "  - le cache intercepte `RESP(A)` et crée une entrée `KEY:A -> RESP(A)` dans son dictionnaire\n",
    "  - le cache renvoie `RESP(A)` au client\n",
    "- si le cache connaît `KEY:A`, alors il renvoie `RESP(A)` au `client` sans interroger le serveur\n",
    "\n",
    "2 cas de figure peuvent justifier le recours à un cache : économiser de l'argent ou du temps.\n",
    "\n",
    "### Caching pour économiser de l'argent\n",
    "Supposons qu'un service distant doive être appelé par une notre archi logicielle. Supposons que \n",
    "- ce service ait un coût ; soit car il nécessite de payer un service tiers à l'appel (exemple: ChatGPT), soit parce que le volume d'appel engendré implique un agrandissement des ressources pour ce service\n",
    "- sa réponse varie peu dans le temps (ie pas un flux vidéo, pas une donnée météo, etc...). Exemple de service distant : un service IA d'embedding de texte en vu d'une recherche vectorielle.\n",
    "\n",
    "Notre architecture peut être amenée à évoluer, des techno peuvent changer, des data re-traitées ... Sans pour autant que les données changent fondamentalement. Dans ce cas, il est préférable d'éviter de payer des appels inutiles au service coûteux s'il faut regénérer ses outputs. Le cache intervient ici en stockant les retours des appels au service tiers une fois pour toute. Le coût d'appel est donc remplacé par un coût de stockage.\n",
    "\n",
    "### Caching pour économiser du temps\n",
    "Supposons qu'une donnée soit stockée en DB, tel le résultat d'une jointure. Supposons que plusieurs services de notre architecture nécessitent ponctuellement la connaissance de cette information. Exemple : connaître les 10 dernières commandes d'un acheteur sur un site e-commerce => les services ayant simultanément besoin de cette info : le front pour affichage, un service ML pour calcul de la probabilité d'achat du client, un service comptable de vérification que toutes les factures sont honorées ... le tout en l'espace de 5 secondes.\n",
    "\n",
    "<img src=\"schemas-MIAGE-no-cache-time.png\" width=\"800\" style=\"display: block; margin: 0 auto\">\n",
    "\n",
    "Chaque appel à la DB a un coût ; diviser la charge de la DB par 2 pourrait permettre de diviser les CPU et la RAM loués par ~2 (estimation grosse maille). Ainsi, la stratégie suivante :\n",
    "- n'appeler la DB qu'une seule fois lorsque le premier service demande l'info\n",
    "- stocker sa réponse dans un cache\n",
    "- économiser les appels DB suivants en distribuant simplement à la réponse cachée aux autres services lorsqu'ils demandent l'info\n",
    "=> Permet de diviser sensiblement la charge sur la DB.\n",
    "\n",
    "<img src=\"schemas-cache-enabled.png\" width=\"800\" style=\"display: block; margin: 0 auto\">\n",
    "\n",
    "## Time to live\n",
    "\n",
    "Un cache peut programmer l'expiration des clefs enregistrées au bout de `x` secondes. Ce délai s'appelle *time to live* ou TTL. Le TTL permet d'établir un compromis entre 2 types de coûts\n",
    "- coût de stockage RAM du cache qui augmente sans fin\n",
    "- coût de rappel ponctuel du service caché\n",
    "\n",
    "Lorsqu'une clef expire, le cache la supprime. Le prochain appel sur cette clef provoquera une interrogation du serveur et permettra de rafraîchir la clef dans le cache.\n",
    "\n",
    "Le TTL permet également de faire expirer une information lorsqu'on sait qu'elle peut périmer après un certain temps. Exemple : dans un moteur de recherche web (Qwant, Google), les recherches \"facebook\", \"youtube\", \"gmail\" sont faites des milliers de fois par seconde. Or il inutile de déclencher la cascade de calcul qui permet d'y répondre à chaque fois - les résultats attendus risquent de ne changer qu'en quelques heures. Ainsi, programmer un TTL de 3600 secondes permet :\n",
    "- d'économiser 3600*1000=3.6M requêtes au moteur\n",
    "- de maintenir les résultats à jour à intervalle régulier\n",
    "\n",
    "\n",
    "## Redis\n",
    "Redis, pour *RE*mote *DI*ctionary *S*erver, est un cache très répandu que nous allons utiliser ici. Voir [la doc](https://redis.io/docs/latest/) pour se rendre compte du nombre d'outils connexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84724b22-00f2-4eee-9be8-15cba8d34eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Redis connection details\n",
    "REDIS_HOST = \"redis\"  # Use \"localhost\" if running Redis locally outside of Docker\n",
    "REDIS_PORT = 6379\n",
    "rcache = redis.Redis(host=REDIS_HOST, port=REDIS_PORT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5def3a18-931a-4c90-923a-6f2cdfa3727d",
   "metadata": {},
   "source": [
    "## Insertion et TTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f65a7-0ea3-4c6c-82c7-18d51e6399f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttm_sec = 5\n",
    "key = \"key:my_first_key\"\n",
    "rcache.set(name=key, value=42, ex=ttm_sec)\n",
    "\n",
    "tic = time.time()\n",
    "time.sleep(1)\n",
    "v = rcache.get(key)\n",
    "toc = time.time() \n",
    "print(f\"After {int(toc-tic)} seconds:\")\n",
    "if v:\n",
    "    print(f\"Recovered value from cache:\", v)\n",
    "print()\n",
    "time.sleep(6)\n",
    "v = rcache.get(key)\n",
    "toc = time.time() \n",
    "print(f\"After {int(toc-tic)} seconds:\")\n",
    "if v:\n",
    "    print(f\"Recovered value from cache:\", v)\n",
    "else:\n",
    "    print(f\"No entry in cache for key {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5228eb9-662a-4c35-816f-7e0f62eda735",
   "metadata": {},
   "source": [
    "## Exercice\n",
    "Réaliser une classe Python pour wrapper un appel à une API externe en essayant prioritairement de trouver l'info sur un cache Redis. Spec :\n",
    "- le constructeur de la classe doit prendre en argument :\n",
    "  - une instance de client Redis\n",
    "  - une instance de client pour l'API externe. On suppose que cette API possède une méthode `get(object_input: str)` qui permet d'appeler le service externe pour l'objet dont l'id est `object_input`\n",
    "  - un TTL\n",
    "- la classe doit exposer une méthode `get(object_input: str)` qui orchestre l'appel API ou REDIS comme expliqué plus haut\n",
    "\n",
    "Squelette:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a23753-0ec4-4e62-af85-46aa146fe0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "# MySQL connection details\n",
    "mysql_host = 'mysql'\n",
    "mysql_user = 'root' # blabla \n",
    "mysql_password = 'rootpassword'\n",
    "mysql_database = 'workshop_db'\n",
    "\n",
    "# Create a connection to the MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host=mysql_host,\n",
    "    user=mysql_user,\n",
    "    password=mysql_password,\n",
    "    database=mysql_database\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de8a338-7ab5-4d91-944b-55a3b28186d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd975342-8f6e-417f-9cd4-ffad7007cf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from rich.progress import track, Progress\n",
    "\n",
    "def batched(iterable, batch_size=16):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, batch_size):\n",
    "        yield iterable[ndx:min(ndx + batch_size, l)]\n",
    "\n",
    "class JinaEmbedder:\n",
    "    \n",
    "    URL = 'https://api.jina.ai/v1/embeddings'\n",
    "    EMBEDDING_NAME = \"jina-embeddings-v2-base-en\"\n",
    "    bearer_token = 'Bearer jina_82bf2b472cd5427a8fc20c6ed47188dfqYajsVcyJBdY7L-ZgYuuTd6GQ5rW'\n",
    "\n",
    "    @staticmethod\n",
    "    def http_json_to_vec(http_json: dict):\n",
    "        return np.array(\n",
    "            [\n",
    "                sentence[\"embedding\"]\n",
    "                for sentence in http_json[\"data\"]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _get_header(cls) -> dict:\n",
    "        return {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Authorization': cls.bearer_token\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def _embed_one_batch(cls, batch: List[str]) -> requests.Response:\n",
    "        headers = cls._get_header()\n",
    "        data = {\n",
    "            'model': cls.EMBEDDING_NAME,\n",
    "            'normalized': True,\n",
    "            'embedding_type': 'float',\n",
    "            'input': batch\n",
    "        }\n",
    "        \n",
    "        return requests.post(cls.URL, headers=headers, json=data)\n",
    "\n",
    "    @classmethod\n",
    "    def embed(cls, str_to_vectorize: List[str] | str, batch_size=256) -> np.ndarray:\n",
    "        if isinstance(str_to_vectorize, str):\n",
    "            str_to_vectorize = [str_to_vectorize]\n",
    "\n",
    "        embeddings = []\n",
    "        with Progress() as progress:\n",
    "            for i, batch in progress.track(enumerate(batched(str_to_vectorize, batch_size=batch_size))):\n",
    "                progress.print(f\"batch {i}...\")\n",
    "                response = cls._embed_one_batch(batch)\n",
    "        \n",
    "                if (sc:=response.status_code) != 200:\n",
    "                    print(\"Warning ! Batch\", i, \"has status code\", sc, \"-> skipping\")\n",
    "                    embeddings.append(np.array([None]*len(batch)))\n",
    "                else:\n",
    "                    embeddings.append(JinaEmbedder.http_json_to_vec(response.json()))\n",
    "        return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b97dd-38ef-4cb7-af30-a2042df1f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedisWrapper:\n",
    "    def __init__(self, redis_client: redis.Redis, jina_client: JinaEmbedder):\n",
    "        self.client: redis.Redis = redis_client\n",
    "        self.jina_client = jina_client\n",
    "\n",
    "    @staticmethod\n",
    "    def get_key_short(beer_id: str) -> str:\n",
    "        return f\"{beer_id}_court\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_key_long(beer_id: str) -> str:\n",
    "        return f\"{beer_id}_long\"\n",
    "\n",
    "    def embed_short(self, beer_id: str, descr: str):\n",
    "        key = self.get_key_short(beer_id)\n",
    "        return self._cached_embed(key, descr)\n",
    "\n",
    "    def embed_long(self, beer_id: str, descr: str):\n",
    "        key = self.get_key_long(beer_id)\n",
    "        return self._cached_embed(key, descr)\n",
    "\n",
    "    def _cached_embed(self, key, descr):\n",
    "        embedding = self.client.get(key)\n",
    "        if embedding:\n",
    "            return np.fromstring(embedding)\n",
    "        else:\n",
    "            embedding = self.jina_client.embed(descr)\n",
    "            self.client.set(name=key, value=embedding.tostring(), ex=60*20)\n",
    "            return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc85596-ccbb-4012-b7d5-1761e134aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_jina = RedisWrapper(rcache, JinaEmbedder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d2dcd-0425-43ce-bd93-f70fd9c6d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "emb = cached_jina.embed_long(12334, \"coucou c'est François\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f854c-7a2b-4165-8313-b6a33f7545bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "q = \"\"\"\n",
    "WITH data AS (\n",
    "    SELECT \n",
    "        beers.id, beers.name, beers.abv, beers.ibu, beers.srm, beers.descript as beer_descr,\n",
    "        brew.descript as brewer_descript, brew.name as brewery,\n",
    "        styles.style_name\n",
    "    FROM beers\n",
    "    LEFT JOIN breweries as brew on brew.id = beers.brewery_id\n",
    "    LEFT JOIN styles on styles.id = beers.style_id\n",
    "), descriptions AS (\n",
    "    SELECT \n",
    "        id,\n",
    "        CONCAT('the beer ', name, ' from brewery ', brewery, ' (', brewer_descript, ') crafts the beer ', name, ' defined as ', beer_descr, '. Spec of the beer are: ABV=', abv, ', IBU=', ibu, ', SRM=', srm) as descr_long,\n",
    "        CONCAT('beer ', name, ': ', beer_descr) as descr_short\n",
    "    FROM data\n",
    ")\n",
    "SELECT \n",
    "    id, descr_short, descr_long\n",
    "FROM descriptions\n",
    "WHERE True\n",
    "    AND id % 12 = 1\n",
    ";\"\"\"\n",
    "df = pd.read_sql_query(q, con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac7e4a-0391-48d3-824a-ca3b73973bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e48929-a30b-4e36-8f98-7ac521db5fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
