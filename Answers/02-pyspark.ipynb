{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Initialize PySpark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JupyterHub PySpark Example\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate() \n",
    "\n",
    "# /!\\ Tout se fera à partir de cet object magique `spark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beers = spark.read.csv(\"/datasets/csv/beers.csv\", header=True)\n",
    "df_breweries = spark.read.csv(\"/datasets/csv/breweries.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=FloatType())\n",
    "def safe_cast_to_float(str_float: str):\n",
    "    return float(str_float)\n",
    "\n",
    "df_beers_brewers = (\n",
    "    df_beers\n",
    "    .join(df_breweries.withColumnRenamed(\"name\", \"brewer_name\"), on=df_beers.brewery_id == df_breweries.id)\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UC-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "n_beers = df_beers.count()\n",
    "print(f\"Q1: {n_beers} dans la DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Q2\")\n",
    "dd = (df_beers\n",
    "      .join(df_breweries, on=df_beers.brewery_id == df_breweries.id)\n",
    "      .groupby(\"country\")\n",
    "      .count()\n",
    "      .sort(F.col(\"count\").desc())\n",
    "      .limit(10)\n",
    ")\n",
    "dd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Q3\n",
    "\n",
    "@F.udf(returnType=FloatType())\n",
    "def safe_cast_to_float(str_float: str):\n",
    "    return float(str_float)\n",
    "\n",
    "df_beers_brewers = (\n",
    "    df_beers\n",
    "    .join(df_breweries.withColumnRenamed(\"name\", \"brewer_name\"), on=df_beers.brewery_id == df_breweries.id)\n",
    ").cache()\n",
    "\n",
    "print(\"Q3\")\n",
    "dd = (df_beers_brewers\n",
    "      .filter(F.col(\"country\") == F.lit(\"France\"))\n",
    "      .withColumn(\"abv_float\", safe_cast_to_float(F.col(\"abv\")))\n",
    "      .sort(F.col(\"abv_float\").desc())\n",
    "      .select([\"name\", \"abv_float\", \"country\"])\n",
    "      .limit(10)\n",
    ")\n",
    "dd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Q4\")\n",
    "df_style = spark.read.csv(\"/datasets/csv/styles.csv\", header=True)\n",
    "target_style_id = df_style.filter(F.lower(F.col(\"style_name\")) == \"porter\").select(F.col(\"id\").alias(\"style_id\"))\n",
    "dd = (\n",
    "    df_beers_brewers\n",
    "    .join(target_style_id, how=\"inner\", on=\"style_id\")\n",
    "    .withColumn(\"abv_float\", safe_cast_to_float(F.col(\"abv\")))\n",
    "    .select([\"name\", \"brewer_name\", \"abv_float\", \"country\"])\n",
    "    .groupby(\"country\")\n",
    "    .agg(F.avg(\"abv_float\").alias(\"avg_abv\"), F.countDistinct(\"brewer_name\").alias(\"n_brewer_having_porter\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Q5\")\n",
    "dd = (\n",
    "    df_beers_brewers\n",
    "    .groupby(\"country\")\n",
    "    .count()\n",
    "    .agg(F.median(\"count\"))\n",
    ")\n",
    "print(\"Q5:\", dd.first()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UC-2\n",
    "Exo difficile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output schema : https://stackoverflow.com/a/54771215/10716281\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "mapping = {\n",
    "    \"float64\": DoubleType,\n",
    "    \"object\":StringType,\n",
    "    \"int64\":IntegerType,\n",
    "    \"int32\":IntegerType,\n",
    "    \"bool\": BooleanType,\n",
    "} # Incomplete - extend with your types.\n",
    "\n",
    "def createUDFSchemaFromPandas(dfp):\n",
    "  column_types  = [StructField(key, mapping[str(dfp.dtypes[key])]()) for key in dfp.columns]\n",
    "  schema = StructType(column_types)\n",
    "  return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cascade_model_per_user_query(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    pos_of_clicked_id = df[df[\"id_in_serp\"] == df[\"clicked_id\"]].iloc[0][\"pos_in_serp\"]\n",
    "    df[\"seen\"] = (df[\"pos_in_serp\"] <= pos_of_clicked_id).astype(int)\n",
    "    df[\"clicked\"] = np.where(df[\"pos_in_serp\"] == pos_of_clicked_id, 1, 0)\n",
    "    return df\n",
    "\n",
    "df_processed = compute_cascade_model_per_user_query(df_pref.limit(3).toPandas())\n",
    "schema = createUDFSchemaFromPandas(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_pref\n",
    "    .repartition(16, \"query\", \"user_id\")\n",
    "    .groupby([\"query\", \"user_id\"])\n",
    "    .applyInPandas(compute_cascade_model_per_user_query, schema)\n",
    "    .filter(F.col(\"seen\") == F.lit(1))\n",
    "    .groupby([\"query\", \"id_in_serp\"])\n",
    "    .agg(F.sum(\"seen\").alias(\"n_seen\"), F.sum(\"clicked\").alias(\"n_clicked\"))\n",
    "    .withColumn(\"clic_proba\", F.col(\"n_clicked\") / F.col(\"n_seen\"))\n",
    "    .select([\"query\", \"id_in_serp\", \"clic_proba\"])\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UC-3 search from a query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation :** On ne pourra pas aller bien loin en terme de souplesse dans la requête"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_description = (\n",
    "    df_beers\n",
    "    .select([\"id\", \"name\", \"descript\", \"brewery_id\"])\n",
    "    .withColumn(\"beer_text\", F.coalesce(F.col(\"descript\"), F.lit(\"\")))\n",
    "    .withColumnRenamed(\"name\", \"beer_name\")\n",
    "    .join(\n",
    "        (\n",
    "            df_breweries\n",
    "            .select([\"id\", \"name\", \"descript\"])\n",
    "            .withColumn(\"brewer_text\", F.coalesce(F.col(\"descript\"), F.lit(\"\")))\n",
    "            .withColumnRenamed(\"name\", \"brewer_name\")\n",
    "        ), \n",
    "        on=df_beers.brewery_id == df_breweries.id\n",
    "    )\n",
    "    .withColumn(\"text\", F.concat(F.col(\"beer_text\"), F.lit(\". \"), F.col(\"brewer_text\")))\n",
    "    .select([\"beer_name\", \"brewer_name\", \"text\"])\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"stout\"\n",
    "df_description.filter(F.col(\"text\").contains(query)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UC-4 vectorize items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "class JinaEmbedder:\n",
    "    \n",
    "    URL = 'https://api.jina.ai/v1/embeddings'\n",
    "    EMBEDDING_NAME = \"jina-embeddings-v2-base-en\"\n",
    "    bearer_token = 'Bearer jina_85ba1ab9e5ff4017b3d216ebb8734f27xzJ9WyoYBFwqks9lOaNLHryw_Yyz'\n",
    "\n",
    "    @staticmethod\n",
    "    def http_json_to_vec(http_json: dict):\n",
    "        return np.array(\n",
    "            [\n",
    "                sentence[\"embedding\"]\n",
    "                for sentence in http_json[\"data\"]\n",
    "            ]\n",
    "        )        \n",
    "\n",
    "    @classmethod\n",
    "    def embed(cls, str_to_vectorize: List[str] | str) -> np.ndarray:\n",
    "        if isinstance(str_to_vectorize, str):\n",
    "            str_to_vectorize = [str_to_vectorize]\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Authorization': cls.bearer_token\n",
    "        }\n",
    "        data = {\n",
    "            'model': cls.EMBEDDING_NAME,\n",
    "            'normalized': True,\n",
    "            'embedding_type': 'float',\n",
    "            'input': str_to_vectorize\n",
    "        }\n",
    "        \n",
    "        response = requests.post(cls.URL, headers=headers, json=data)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            return None\n",
    "\n",
    "        return JinaEmbedder.http_json_to_vec(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(StringType())\n",
    "def craft_to_txt_vectorize(beer_name, brewer_name, beer_text, brewer_text, abv, ibu, srm):\n",
    "    beer_name = beer_name if beer_name else \"\"\n",
    "    brewer_name = brewer_name if brewer_name else \"\"\n",
    "    beer_text = beer_text if beer_text else \"\"\n",
    "    brewer_text = brewer_text if brewer_text else \"\"\n",
    "    abv = abv if abv else \"\"\n",
    "    ibu = ibu if ibu else \"\"\n",
    "    srm = srm if srm else \"\"\n",
    "    return f\"The rewery {brewer_name} ({brewer_text}) brews {beer_name} which is described as {beer_text}. Spec of the beer is {abv=}, {ibu=}, {srm=}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_description = (\n",
    "    df_beers.alias(\"beer\")\n",
    "    .withColumn(\"beer_text\", F.coalesce(F.col(\"descript\"), F.lit(\"\")))\n",
    "    .withColumnRenamed(\"name\", \"beer_name\")\n",
    "    .join(\n",
    "        (\n",
    "            df_breweries\n",
    "            .select([\"id\", \"name\", \"descript\"])\n",
    "            .withColumn(\"brewer_text\", F.coalesce(F.col(\"descript\"), F.lit(\"\")))\n",
    "            .withColumnRenamed(\"name\", \"brewer_name\")\n",
    "        ), \n",
    "        on=df_beers.brewery_id == df_breweries.id\n",
    "    )\n",
    "    .select([\"beer.id\", \"beer_name\", \"brewer_name\", \"beer_text\", \"brewer_text\", \"abv\", \"ibu\", \"srm\"])\n",
    ").cache()\n",
    "\n",
    "df_vec = (\n",
    "    df_description\n",
    "    .withColumn(\"to_vec\", craft_to_txt_vectorize(\"beer_name\", \"brewer_name\", \"beer_text\", \"brewer_text\", \"abv\", \"ibu\", \"srm\"))\n",
    "    .repartition(2)\n",
    "    .select(\"beer.id\", \"to_vec\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(iterator):\n",
    "    ids = []\n",
    "    texts = []\n",
    "    for row in iterator:\n",
    "        ids.append(row.id)\n",
    "        texts.append(row.to_vec)\n",
    "    embeddings = JinaEmbedder.embed(texts)\n",
    "    for _id, embedding in zip(ids, embeddings):\n",
    "        yield _id, embedding.tolist()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"embedding\", ArrayType(FloatType()), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_df = (\n",
    "    df_vec\n",
    "    .filter(F.col(\"id\") % F.lit(12*12) == 3)\n",
    "    .rdd.mapPartitions(my_func)\n",
    "    .toDF(schema).cache()\n",
    "    )\n",
    "vectorized_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque:** on aurait pu ne pas `mapPartition` ou ne pas passer par la fontion tampon `my_func` MAIS on n'aurait pas appelé Jina en batch => cela aurait tué les perfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UC-5 : answer question in corpa\n",
    "Pas vraiment de possibilité native en SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"very bitter beer with smoky taste\", \"fruity sour - balanced sourness\", \"weird beer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|              to_vec|\n",
      "+---+--------------------+\n",
      "|100|The rewery Nebras...|\n",
      "|529|The rewery Brasse...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_corpus = (\n",
    "    df_description\n",
    "    .withColumn(\"to_vec\", craft_to_txt_vectorize(\"beer_name\", \"brewer_name\", \"beer_text\", \"brewer_text\", \"abv\", \"ibu\", \"srm\"))\n",
    "    .repartition(2)\n",
    "    .select(\"id\", \"to_vec\")\n",
    ")\n",
    "df_corpus.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "def create_tf_and_idf_from_corpus(df_corpus):\n",
    "    tf = HashingTF(inputCol=\"tokenized\", outputCol=\"raw_features\")\n",
    "    df = tf.transform(df_corpus.withColumn(\"tokenized\", F.split(\"to_vec\", \" \")))\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\").fit(df)\n",
    "    return tf, idf\n",
    "\n",
    "def turn_to_tf_idf(df_docs, tf, idf):\n",
    "    docs_tf = tf.transform(df_docs.withColumn(\"tokenized\", F.split(\"to_vec\", \" \")))\n",
    "    docs_tfidf = idf.transform(docs_tf)\n",
    "    return docs_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Pre compute TF and IDF\n",
    "tf, idf = create_tf_and_idf_from_corpus(df_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply on corpus and queries\n",
    "corpus_tfidf = turn_to_tf_idf(df_corpus, tf, idf)\n",
    "\n",
    "queries_df = spark.createDataFrame(pd.DataFrame(data={\"to_vec\": queries}))\n",
    "queries_tfidf = turn_to_tf_idf(queries_df, tf, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/14 08:01:40 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# broadcast queries to every partitions\n",
    "broadcast_queries_tfidf = spark.sparkContext.broadcast(queries_tfidf.select([\"features\", \"tokenized\"]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to compute dot product\n",
    "def compute_dot_product(doc_vec, query_vec):\n",
    "    #if isinstance(doc_vec, SparseVector):\n",
    "    doc_vec = doc_vec.toArray()\n",
    "    #if isinstance(query_vec, SparseVector):\n",
    "    query_vec = query_vec.toArray()\n",
    "    return float(doc_vec.dot(query_vec))\n",
    "\n",
    "# Register UDF for dot product\n",
    "dot_product_udf = F.udf(compute_dot_product, DoubleType())\n",
    "\n",
    "# Explode the broadcasted queries_df into rows\n",
    "queries_rdd = spark.sparkContext.parallelize(broadcast_queries_tfidf.value)\n",
    "queries_df_expanded = spark.createDataFrame(queries_rdd)\n",
    "\n",
    "# Cross join docs_df with expanded queries_df\n",
    "cross_joined_df = corpus_tfidf.alias(\"corpus\").crossJoin(queries_df_expanded.alias(\"queries\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/14 08:01:53 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "24/10/14 08:01:55 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "[Stage 64:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|  id|              to_vec|           tokenized|        raw_features|            features|            features|           tokenized|       dot_product|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "| 100|The rewery Nebras...|[The, rewery, Neb...|(262144,[7058,760...|(262144,[7058,760...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "| 529|The rewery Brasse...|[The, rewery, Bra...|(262144,[7606,812...|(262144,[7606,812...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "| 686|The rewery Elysia...|[The, rewery, Ely...|(262144,[7606,187...|(262144,[7606,187...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "| 674|The rewery Big Ti...|[The, rewery, Big...|(262144,[1641,462...|(262144,[1641,462...|(262144,[66208,12...|[very, bitter, be...|1.6752886309867447|\n",
      "| 302|The rewery Jack's...|[The, rewery, Jac...|(262144,[7606,277...|(262144,[7606,277...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "| 510|The rewery Barfer...|[The, rewery, Bar...|(262144,[7606,129...|(262144,[7606,129...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|5023|The rewery Bube's...|[The, rewery, Bub...|(262144,[336,7123...|(262144,[336,7123...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|3849|The rewery Otto's...|[The, rewery, Ott...|(262144,[4629,760...|(262144,[4629,760...|(262144,[66208,12...|[very, bitter, be...|13.885886163576961|\n",
      "|2436|The rewery Main S...|[The, rewery, Mai...|(262144,[7606,134...|(262144,[7606,134...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|3532|The rewery Faultl...|[The, rewery, Fau...|(262144,[7606,277...|(262144,[7606,277...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|4290|The rewery Calder...|[The, rewery, Cal...|(262144,[1828,760...|(262144,[1828,760...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|3815|The rewery Yuengl...|[The, rewery, Yue...|(262144,[3564,462...|(262144,[3564,462...|(262144,[66208,12...|[very, bitter, be...| 5.025865892960234|\n",
      "|3088|The rewery Goose ...|[The, rewery, Goo...|(262144,[7606,277...|(262144,[7606,277...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|4435|The rewery Steamw...|[The, rewery, Ste...|(262144,[287,1603...|(262144,[287,1603...|(262144,[66208,12...|[very, bitter, be...|3.3505772619734895|\n",
      "|4679|The rewery InBev ...|[The, rewery, InB...|(262144,[424,619,...|(262144,[424,619,...|(262144,[66208,12...|[very, bitter, be...|1.6752886309867447|\n",
      "|2409|The rewery Charli...|[The, rewery, Cha...|(262144,[7606,277...|(262144,[7606,277...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|2605|The rewery Asahi ...|[The, rewery, Asa...|(262144,[7606,195...|(262144,[7606,195...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "| 344|The rewery Little...|[The, rewery, Lit...|(262144,[7606,277...|(262144,[7606,277...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|1356|The rewery Bridge...|[The, rewery, Bri...|(262144,[7606,277...|(262144,[7606,277...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "|2512|The rewery Bull &...|[The, rewery, Bul...|(262144,[7606,277...|(262144,[7606,277...|(262144,[66208,12...|[very, bitter, be...|               0.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/14 08:02:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Apply the dot product UDF\n",
    "result_df = cross_joined_df.withColumn(\n",
    "    \"dot_product\",\n",
    "    dot_product_udf(cross_joined_df[\"corpus.features\"], cross_joined_df[\"queries.features\"])\n",
    ").cache()\n",
    "\n",
    "# Show result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/14 08:02:20 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|  id|              to_vec|           tokenized|        raw_features|            features|            features|           tokenized|       dot_product|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|5518|The rewery Sabmil...|[The, rewery, Sab...|(262144,[7058,760...|(262144,[7058,760...|(262144,[66208,12...|[very, bitter, be...|105.79780515834159|\n",
      "|3602|The rewery Boston...|[The, rewery, Bos...|(262144,[336,1578...|(262144,[336,1578...|(262144,[16704,27...|[fruity, sour, -,...| 83.91526396115003|\n",
      "|5171|The rewery Roy Pi...|[The, rewery, Roy...|(262144,[991,1581...|(262144,[991,1581...|(262144,[66208,12...|[very, bitter, be...| 78.46171430817284|\n",
      "| 439|The rewery New Be...|[The, rewery, New...|(262144,[535,3456...|(262144,[535,3456...|(262144,[16704,27...|[fruity, sour, -,...|  76.8548726479598|\n",
      "|4282|The rewery Allaga...|[The, rewery, All...|(262144,[303,336,...|(262144,[303,336,...|(262144,[66208,12...|[very, bitter, be...| 64.40356492492458|\n",
      "|4448|The rewery Storm ...|[The, rewery, Sto...|(262144,[1828,365...|(262144,[1828,365...|(262144,[56718,12...|       [weird, beer]| 63.85202462101636|\n",
      "|5733|The rewery Saint ...|[The, rewery, Sai...|(262144,[4629,760...|(262144,[4629,760...|(262144,[66208,12...|[very, bitter, be...| 58.72209867791031|\n",
      "|4422|The rewery Brouwe...|[The, rewery, Bro...|(262144,[7606,114...|(262144,[7606,114...|(262144,[16704,27...|[fruity, sour, -,...| 58.52173674028139|\n",
      "| 481|The rewery Brouwe...|[The, rewery, Bro...|(262144,[1575,219...|(262144,[1575,219...|(262144,[16704,27...|[fruity, sour, -,...| 58.49067566411356|\n",
      "|4676|The rewery The Ch...|[The, rewery, The...|(262144,[4629,760...|(262144,[4629,760...|(262144,[66208,12...|[very, bitter, be...| 54.35183313900411|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.sort(\"dot_product\", ascending=False).limit(10).show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
